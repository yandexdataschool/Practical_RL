{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["### Practical_RL week3 homework\n", "\n", "In this notebook we'll get more perspective on how value-based methods work. \n", "\n", "We assume that you've already done either seminar_main or seminar_alternative.\n", "\n", "To begin with, __please edit qlearning.py__ - just copy your implementation from the first part of this assignment."]}, {"cell_type": "code", "execution_count": 1, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Starting virtual X frame buffer: Xvfb../xvfb: line 8: start-stop-daemon: command not found\n", ".\n", "env: DISPLAY=:1\n"]}], "source": ["%load_ext autoreload", "\n", "%autoreload 2", "\n", "import numpy as np", "\n", "import matplotlib.pyplot as plt", "\n", "%matplotlib inline", "\n", "from IPython.display import clear_output", "\n", "\n", "# XVFB will be launched if you run on a server", "\n", "import os", "\n", "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:", "\n", "    !bash ../xvfb start", "\n", "    %env DISPLAY = : 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Q-learning in the wild (3 pts)\n", "\n", "Here we use the qlearning agent on taxi env from openai gym.\n", "You will need to insert a few agent functions here."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["import gym", "\n", "env = gym.make(\"Taxi-v2\")", "\n", "\n", "n_actions = env.action_space.n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["from cs188.qlearningAgents import QLearningAgent", "\n", "\n", "agent = QLearningAgent(alpha=0.5, epsilon=0.25, gamma=0.99,", "\n", "                       actionFn=lambda s: range(n_actions))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["def play_and_train(env, agent, t_max=10**4):", "\n", "    \"\"\"This function should ", "\n", "    - run a full game, actions given by agent.getAction(s)", "\n", "    - train agent using agent.update(...) whenever possible", "\n", "    - return total reward\"\"\"", "\n", "    total_reward = 0.0", "\n", "    s = env.reset()", "\n", "\n", "    for t in range(t_max):", "\n", "        a =  # <get agent to pick action given state s>", "\n", "\n", "        next_s, r, done, _ = env.step(a)", "\n", "\n", "        #<train (update) agent for state s>", "\n", "\n", "        s = next_s", "\n", "        total_reward += r", "\n", "        if done:", "\n", "            break", "\n", "\n", "    return total_reward"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["rewards = []", "\n", "for i in range(1000):", "\n", "    rewards.append(play_and_train(env, agent))", "\n", "    if i % 100 == 0:", "\n", "        clear_output(True)", "\n", "        print \"mean reward\", np.mean(rewards[-100:])", "\n", "        plt.plot(rewards)", "\n", "        plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.1 reducing epsilon\n", "\n", "Try decreasing agent epsilon over time to make him reach positive score.\n", "\n", "The straightforward way to do so is to reduce epsilon every N games:\n", "* either multiply agent.epsilon by a number less than 1 (e.g. 0.99)\n", "* or substract a small value until it reaches 0\n", "\n", "You can, of-course, devise other strategies.\n", "\n", "__The goal is to reach positive reward!__"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Expected value SARSA (1 pt)\n", "\n", "Let's try out expected-value SARSA. You will have to implement EV-SARSA as an agent, resembling the one you used in qlearning.py ,\n", "\n", "```<go to expected_value_sarsa.py and implement missing lines in getValue(state)```\n", "\n", "__[bonus, 2pt]__ implement EV-SARSA for softmax policy:\n", "\n", "$$ \\pi(a_i|s) = softmax({Q(s,a_i) \\over \\tau}) = {e ^ {Q(s,a_i)/ \\tau}  \\over {\\sum_{a_j}  e ^{Q(s,a_j) / \\tau }}} $$"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["import gym", "\n", "env = gym.make(\"Taxi-v2\")", "\n", "\n", "n_actions = env.action_space.n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["from expected_value_sarsa import EVSarsaAgent", "\n", "agent = EVSarsaAgent(alpha=0.5, epsilon=0.25, gamma=0.99,", "\n", "                     actionFn=lambda s: range(n_actions))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Train EV-SARSA\n", "\n", "Note that it uses __the same update parameters as__ qlearning so you adapt use the ```play_and_train``` code above.\n", "\n", "Please try both constant epsilon = 0.25 and decreasing epsilon."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["<your code here >"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["## 3. Continuous state space (2 pt)\n", "\n", "Use agent to train on CartPole-v0\n", "\n", "This environment has a continuous number of states, so you will have to group them into bins somehow.\n", "\n", "The simplest way is to use `round(x,n_digits)` (or numpy round) to round real number to a given amount of digits.\n", "\n", "The tricky part is to get the n_digits right for each state to train effectively.\n", "\n", "Note that you don't need to convert state to integers, but to __tuples__ of any kind of values."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["env = gym.make(\"CartPole-v0\")", "\n", "n_actions = env.action_space.n", "\n", "\n", "print(\"first state:%s\" % (env.reset()))", "\n", "plt.imshow(env.render('rgb_array'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Play a few games\n", "\n", "We need to estimate observation distributions. To do so, we'll play a few games and record all states."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["all_states = []", "\n", "for _ in range(1000):", "\n", "    all_states.append(env.reset())", "\n", "    done = False", "\n", "    while not done:", "\n", "        s, r, done, _ = env.step(env.action_space.sample())", "\n", "        all_states.append(s)", "\n", "        if done:", "\n", "            break", "\n", "\n", "all_states = np.array(all_states)", "\n", "\n", "for obs_i in range(env.observation_space.shape[0]):", "\n", "\n", "    plt.hist(all_states[:, obs_i], bins=20)", "\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["rewards = []", "\n", "for i in range(1000):", "\n", "    rewards.append(play_and_train(env, agent))", "\n", "    if i % 100 == 0:", "\n", "        clear_output(True)", "\n", "        print \"mean reward\", np.mean(rewards[-100:])", "\n", "        plt.plot(rewards)", "\n", "        plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Binarize environment"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["from gym.core import ObservationWrapper", "\n", "\n", "\n", "class Binarizer(ObservationWrapper):", "\n", "\n", "    def _observation(self, state):", "\n", "\n", "        # state = <round state to some amount digits.>", "\n", "        # hint: you can do that with round(x,n_digits)", "\n", "        # you will need to pick a different n_digits for each dimension", "\n", "\n", "        return tuple(state)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["env = Binarizer(gym.make(\"CartPole-v0\"))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["all_states = []", "\n", "for _ in range(1000):", "\n", "    all_states.append(env.reset())", "\n", "    done = False", "\n", "    while not done:", "\n", "        s, r, done, _ = env.step(env.action_space.sample())", "\n", "        all_states.append(s)", "\n", "        if done:", "\n", "            break", "\n", "\n", "all_states = np.array(all_states)", "\n", "\n", "for obs_i in range(env.observation_space.shape[0]):", "\n", "\n", "    plt.hist(all_states[:, obs_i], bins=20)", "\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Learn"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["agent = QLearningAgent(alpha=0.5, epsilon=0.25, gamma=0.99,", "\n", "                       actionFn=lambda s: range(n_actions))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["rewards = []", "\n", "for i in range(1000):", "\n", "    rewards.append(play_and_train(env, agent))", "\n", "    if i % 100 == 0:", "\n", "        clear_output(True)", "\n", "        print \"mean reward\", np.mean(rewards[-100:])", "\n", "        plt.plot(rewards)", "\n", "        plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3.2 EV-sarsa on CartPole\n", "\n", "Now train the `EVSarsaAgent` on CartPole-v0 env with binarizer you used above for Q-learning."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["env = <make env and wrap it with binarizer >", "\n", "\n", "agent = <your code >"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["<train me >"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Experience replay (4 pts)\n", "\n", "There's a powerful technique that you can use to improve sample efficiency for off-policy algorithms: [spoiler] Experience replay :)\n", "\n", "The catch is that you can train Q-learning and EV-SARSA on `<s,a,r,s'>` tuples even if they aren't sampled under current agent's policy. So here's what we're gonna do:\n", "\n", "#### Training with experience replay\n", "1. Play game, sample `<s,a,r,s'>`.\n", "2. Update q-values based on `<s,a,r,s'>`.\n", "3. Store `<s,a,r,s'>` transition in a buffer. \n", " 3. If buffer is full, delete earliest data.\n", "4. Sample K such transitions from that buffer and update q-values based on them.\n", "\n", "\n", "To enable such training, first we must implement a memory structure that would act like such a buffer."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["import random", "\n", "\n", "\n", "class ReplayBuffer(object):", "\n", "    def __init__(self, size):", "\n", "        \"\"\"Create Replay buffer.", "\n", "        Parameters", "\n", "        ----------", "\n", "        size: int", "\n", "            Max number of transitions to store in the buffer. When the buffer", "\n", "            overflows the old memories are dropped.", "\n", "        \"\"\"", "\n", "        self._storage = []", "\n", "        self._maxsize = size", "\n", "        <any other vars >", "\n", "\n", "    def __len__(self):", "\n", "        return len(self._storage)", "\n", "\n", "    def add(self, obs_t, action, reward, obs_tp1, done):", "\n", "        '''", "\n", "        Make sure, _storage will not exceed _maxsize. ", "\n", "        Make sure, FIFO rule is being followed: the oldest examples has to be removed earlier", "\n", "        '''", "\n", "        data = (obs_t, action, reward, obs_tp1, done)", "\n", "        <add data to storage. >", "\n", "\n", "    def sample(self, batch_size):", "\n", "        \"\"\"Sample a batch of experiences.", "\n", "        Parameters", "\n", "        ----------", "\n", "        batch_size: int", "\n", "            How many transitions to sample.", "\n", "        Returns", "\n", "        -------", "\n", "        obs_batch: np.array", "\n", "            batch of observations", "\n", "        act_batch: np.array", "\n", "            batch of actions executed given obs_batch", "\n", "        rew_batch: np.array", "\n", "            rewards received as results of executing act_batch", "\n", "        next_obs_batch: np.array", "\n", "            next set of observations seen after executing act_batch", "\n", "        done_mask: np.array", "\n", "            done_mask[i] = 1 if executing act_batch[i] resulted in", "\n", "            the end of an episode and 0 otherwise.", "\n", "        \"\"\"", "\n", "        idxes = <randomly generate indexes of samples >", "\n", "\n", "        # Your code: collect <s,a,r,s',done> for each index", "\n", "\n", "        return np.array( < states > ), np.array( < actions > ), np.array( < rewards > ), np.array( < next_states > ), np.array( < is_done > )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Some tests to make sure your buffer works right"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["replay = ReplayBuffer(2)", "\n", "obj1 = tuple(range(5))", "\n", "obj2 = tuple(range(5, 10))", "\n", "replay.add(*obj1)", "\n", "assert replay.sample(", "\n", "    1) == obj1, \"If there's just one object in buffer, it must be retrieved by buf.sample(1)\"", "\n", "replay.add(*obj2)", "\n", "assert len(", "\n", "    replay._storage) == 2, \"Please make sure __len__ methods works as intended.\"", "\n", "replay.add(*obj2)", "\n", "assert len(replay._storage) == 2, \"When buffer is at max capacity, replace objects instead of adding new ones.\"", "\n", "assert tuple(np.unique(a) for a in replay.sample(100)) == obj2", "\n", "replay.add(*obj1)", "\n", "assert max(len(np.unique(a)) for a in replay.sample(100)) == 2", "\n", "replay.add(*obj1)", "\n", "assert tuple(np.unique(a) for a in replay.sample(100)) == obj1", "\n", "print \"Success!\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's use this buffer to improve training:"]}, {"cell_type": "code", "execution_count": 1, "metadata": {"collapsed": false}, "outputs": [{"ename": "NameError", "evalue": "name 'ReplayBuffer' is not defined", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-1-744fe061a1bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mNameError\u001b[0m: name 'ReplayBuffer' is not defined"]}], "source": ["agent = <create agent >", "\n", "replay = ReplayBuffer(10000)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["def play_and_train(env, agent, replay, t_max=10**4):", "\n", "    \"\"\"This function should ", "\n", "    - run a full game, actions given by agent.getAction(s)", "\n", "    - train agent using agent.update(...) whenever possible", "\n", "    - return total reward\"\"\"", "\n", "    total_reward = 0.0", "\n", "    s = env.reset()", "\n", "    <How you need to modify pipeline in order to use ER?>", "\n", "    for t in range(t_max):", "\n", "        a =  # <get agent to pick action given state s>", "\n", "\n", "        next_s, r, done, _ = env.step(a)", "\n", "\n", "        # Your code here: store current <s,a,r,s'> transition in buffer", "\n", "        # Your code here: train on both current", "\n", "\n", "        s = next_s", "\n", "        total_reward += r", "\n", "        if done:", "\n", "            break", "\n", "\n", "    return total_reward"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["Train with experience replay"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Your code:", "\n", "# - build a training loop", "\n", "# - plot learning curves", "\n", "<... >"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Bonus I: TD($ \\lambda $) (5+ points)\n", "\n", "There's a number of advanced algorithms you can find in week 3 materials (Silver lecture II and/or reading about eligibility traces). One such algorithm is TD(lambda), which is based on the idea of eligibility traces. You can also view it as a combination of N-step updates for alll N.\n", "* N-step temporal difference from Sutton's book - [url](http://incompleteideas.net/sutton/book/ebook/node73.html)\n", "* Eligibility traces from Sutton's book - [url](http://incompleteideas.net/sutton/book/ebook/node72.html)\n", "* Blog post on eligibility traces - [url](http://pierrelucbacon.com/traces/)\n", "\n", "\n", "\n", "Implementing this algorithm will prove more challenging than q-learning or sarsa, but doing so will earn you a deeper understanding of how value-based methods work [in addition to some bonus points].\n", "\n", "More kudos for comparing and analyzing TD($\\lambda$) against Q-learning and EV-SARSA in different setups (taxi vs cartpole, constant epsilon vs decreasing epsilon)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Bonus II: More pacman (5+ points)\n", "\n", "Remember seminar_main where your vanilla q-learning had hard time solving Pacman even on a small grid. Now's the time to fix that issue.\n", "\n", "We'll focus on those grids for pacman setup.\n", "* python pacman.py -p PacmanQAgent -x N_TRAIN_GAMES -n N_TOTAL_GAMES -l __mediumGrid__\n", "* python pacman.py -p PacmanQAgent -x N_TRAIN_GAMES -n N_TOTAL_GAMES -l __mediumClassic__\n", "\n", "Even if you adjust N_TRAIN_GAMES to 10^5 and N_TOTAL_GAMES to 10^5+100 (100 last games are for test), pacman won't solve those environments\n", "\n", "The problem with those environments is that they have a large amount of unique states. However, you can devise a smaller environment state by choosing different observation parameters, e.g.:\n", " * distance and direction to nearest ghost\n", " * where is nearest food\n", " * 'center of mass' of all food points (and variance, and whatever)\n", " * is there a wall in each direction\n", " * and anything else you see fit \n", " \n", "Here's how to get this information from [state](https://github.com/yandexdataschool/Practical_RL/blob/master/week2/assignment/pacman.py#L49),\n", " * Get pacman position: [state.getPacmanPosition()](https://github.com/yandexdataschool/Practical_RL/blob/master/week2/assignment/pacman.py#L128)\n", " * Is there a wall at (x,y)?: [state.hasWall(x,y)](https://github.com/yandexdataschool/Practical_RL/blob/master/week2/assignment/pacman.py#L189)\n", " * Get ghost positions: [state.getGhostPositions()](https://github.com/yandexdataschool/Practical_RL/blob/master/week2/assignment/pacman.py#L144)\n", " * Get all food positions: [state.getCapsules()](https://github.com/yandexdataschool/Practical_RL/blob/master/week2/assignment/pacman.py#L153)\n", " \n", "You can call those methods anywhere you see state.\n", " * e.g. in [agent.getValue(state)](https://github.com/yandexdataschool/Practical_RL/blob/master/week2/assignment/qlearningAgents.py#L52)\n", " * Defining a function that extracts all features and calling it in [getQValue](https://github.com/yandexdataschool/Practical_RL/blob/master/week2/assignment/qlearningAgents.py#L38) and [setQValue](https://github.com/yandexdataschool/Practical_RL/blob/master/week2/assignment/qlearningAgents.py#L44) is probably enough.\n", " * You can also change agent parameters. The simplest way is to hard-code them in [PacmanQAgent](https://github.com/yandexdataschool/Practical_RL/blob/master/week2/assignment/qlearningAgents.py#L140)\n", "\n", "Also, don't forget to optimize ```learning_rate```, ```discount``` and ```epsilon``` params of model, this may also help to solve this env."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 2", "language": "python", "name": "python2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 2}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython2", "version": "2.7.13"}}, "nbformat": 4, "nbformat_minor": 1}