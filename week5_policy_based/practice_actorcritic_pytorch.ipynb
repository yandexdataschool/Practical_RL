{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Kung-Fu with advantage actor-critic\n",
    "\n",
    "In this notebook you'll build a deep reinforcement learning agent for Atari [Kung-Fu Master](https://gym.openai.com/envs/KungFuMaster-v0/) and train it with Advantage Actor-Critic.\n",
    "\n",
    "Note that, strictly speaking, this will be neither [A3C](https://arxiv.org/abs/1602.01783) nor [A2C](https://openai.com/blog/baselines-acktr-a2c/), but rather a simplified version of the latter.\n",
    "\n",
    "Special thanks to Jesse Grabowski for making an [initial version](https://www.coursera.org/learn/practical-rl/discussions/all/threads/6iDjkbhPQoGg45G4T7KBHQ/replies/5eM_hA7vEeuoKgpcLmLqdw) of the PyTorch port of this assignment.\n",
    "\n",
    "![https://upload.wikimedia.org/wikipedia/en/6/66/Kung_fu_master_mame.png](https://upload.wikimedia.org/wikipedia/en/6/66/Kung_fu_master_mame.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    %tensorflow_version 1.x\n",
    "    \n",
    "    if not os.path.exists('.setup_complete'):\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/atari_util.py\n",
    "\n",
    "        !touch .setup_complete\n",
    "\n",
    "# If you are running on a server, launch xvfb to record game videos\n",
    "# Please make sure you have xvfb installed\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's take a look at the game itself:\n",
    "\n",
    "* Image resized to 42x42 and converted to grayscale to run faster\n",
    "* Agent sees last 4 frames of game to account for object velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(\n",
    "        env, height=42, width=42,\n",
    "        crop=lambda img: img[60:-30, 5:],\n",
    "        dim_order='pytorch',\n",
    "        color=False, n_frames=4)\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation (4-frame buffer)')\n",
    "plt.imshow(s.transpose([1, 0, 2]).reshape([42, -1]), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a network\n",
    "\n",
    "We now have to build an agent for actor-critic training â€” a convolutional neural network, that converts states into action probabilities $\\pi$ and state values $V$.\n",
    "\n",
    "Your assignment here is to build and apply the neural network. You can use any framework you want, but in this notebook we prepared for you a template in PyTorch.\n",
    "\n",
    "For starters, we want you to implement this architecture:\n",
    "\n",
    "![https://s17.postimg.cc/orswlfzcv/nnet_arch.png](https://s17.postimg.cc/orswlfzcv/nnet_arch.png)\n",
    "\n",
    "Notes:\n",
    "* This diagram was originally made for Tensorflow. In PyTorch, the input shape is `[batch_size, 4, 42, 42]`.\n",
    "* Use convolution kernel size 3x3 throughout.\n",
    "* After your agent gets mean reward above 5000, we encourage you to experiment with the model architecture to score even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    Helper function to compute the spatial dimensions of the output\n",
    "    of a convolutional layer, copied from Week 4.\n",
    "\n",
    "    Common use case:\n",
    "        cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
    "        cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
    "    This can be used to understand the shape for the dense layer's input.\n",
    "    \"\"\"\n",
    "    return (size - (kernel_size - 1) - 1) // stride + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, input_dims, n_actions, lr):\n",
    "        super(Agent, self).__init__()\n",
    "        \n",
    "        self.input_dims = input_dims\n",
    "\n",
    "        # Initialize layers as shown in the image above\n",
    "        <YOUR CODE>\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        # Compute logits and values using network.\n",
    "        # Note, that if you do so naively, your state_values will have shape\n",
    "        # ending in 1, since they come from a Linear(..., 1) layer. It is useful\n",
    "        # to .squeeze(dim=-1) them, since this will help to avoid shape conflicts\n",
    "        # in the loss function part, after we add multiple environments.\n",
    "        # If you don't do this here, don't forget to do that in the\n",
    "        # loss function!\n",
    "        \n",
    "        <YOUR CODE>\n",
    "\n",
    "        return logits, state_values\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # PyTorch wants a batch dimension, so if we feed only a single observation we need to wrap it with an extra layer.\n",
    "        # This line will allow the network to handle both single and multi-environment tests.\n",
    "        if observation.ndim == 3:\n",
    "            observation = [observation]\n",
    "\n",
    "        observation = torch.tensor(observation, dtype=torch.float32, device=device)\n",
    "        # Pass states into the agent network and get back logits and states\n",
    "        logits, _ = <YOUR CODE>\n",
    "                \n",
    "        policy = F.softmax(logits, dim=-1)\n",
    "\n",
    "        actions = np.array([np.random.choice(len(p), p=p) for p in policy.detach().cpu().numpy()])\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test network\n",
    "\n",
    "agent = Agent(input_dims=obs_shape, n_actions=n_actions, lr=1e-4)\n",
    "state = env.reset()\n",
    "state = torch.tensor([state], dtype=torch.float32, device=device)\n",
    "logits, state_values = agent(state)\n",
    "\n",
    "assert isinstance(logits, torch.Tensor) and len(logits.shape) == 2, \\\n",
    "    \"Please return a 2D Torch tensor of logits with shape (batch_size, n_actions). You returned %s\" % repr(logits)\n",
    "assert isinstance(state_values, torch.Tensor) and len(state_values.shape) == 1, \\\n",
    "    \"Please return a 1D Torch tensor of state values with shape (batch_size,). You returned %s\" % repr(state_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Actor-Critic\n",
    "\n",
    "Here we define loss functions and learning algorithms as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_actor_critic_loss(agent, state, action, reward, next_state, done,\n",
    "                              gamma=0.99):\n",
    "    # Infer batch_size from shape of state tensor:\n",
    "    batch_size = state.shape[0]\n",
    "\n",
    "    # Convert everything to a tensor, send to GPU if available\n",
    "    state      = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "    reward     = torch.tensor(reward, dtype=torch.float32, device=device)\n",
    "    done       = torch.tensor(done, dtype=torch.bool, device=device)\n",
    "\n",
    "    # logits[n_envs, n_actions] and state_values[n_envs, n_actions]\n",
    "    logits, state_value = agent(state)\n",
    "    next_logits, next_state_value = agent(next_state)\n",
    "\n",
    "    # Probabilities and log-probabilities for all actions\n",
    "    probs    = F.softmax(logits, dim=-1)     #[n_envs, n_actions]\n",
    "    logprobs = F.log_softmax(logits, dim=-1) #[n_envs, n_actions]\n",
    "\n",
    "    # Set new state values with done == 1 to be 0.0 (no future rewards if done!)\n",
    "    next_state_value[done] = 0.0\n",
    "\n",
    "    # Compute target state values using temporal difference formula.\n",
    "    # Use reward, gamma, and next_state_value.\n",
    "    target_state_value = <YOUR CODE>\n",
    "\n",
    "    # Compute advantage using reward, gamma, state_value, and next_state_value.\n",
    "    advantage = <YOUR CODE>\n",
    "\n",
    "    # We need to slice out only the actions we took for actor loss -- we can use\n",
    "    # the actions themselves as indexes, but we also need indexes on the batch dim\n",
    "    batch_idx = np.arange(batch_size)\n",
    "    logp_actions = logprobs[batch_idx, action]\n",
    "\n",
    "    # Compute policy entropy given logits_seq. Mind the \"-\" sign!\n",
    "    entropy = <YOUR CODE>\n",
    "\n",
    "    actor_loss = -(logp_actions * advantage.detach()).mean() - 0.001 * entropy.mean()\n",
    "    critic_loss = F.mse_loss(target_state_value.detach(), state_value)\n",
    "\n",
    "    total_loss = actor_loss + critic_loss\n",
    "\n",
    "    # Never forget to zero grads in PyTorch!\n",
    "    agent.optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    agent.optimizer.step()\n",
    "\n",
    "    return actor_loss.cpu().detach().numpy(), critic_loss.cpu().detach().numpy(), entropy.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = torch.tensor([state], dtype=torch.float32).to(device)\n",
    "logits, value = agent(state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function, that measures the agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays the game from start till done, returns per-game rewards \"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        state = env.reset()\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = agent.choose_action(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(make_env(), directory=\"videos\", force=True) as env_monitor:\n",
    "    rewards = evaluate(agent, env_monitor, n_games=3)\n",
    "\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-1]))  # You can also try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "\n",
    "![img](https://s7.postimg.cc/4y36s2b2z/env_pool.png)\n",
    "\n",
    "\n",
    "To make actor-critic training more stable, we shall play several games in parallel. This means you'll have to initialize several parallel gym envs, send agent's actions there and .reset() each env if it becomes terminated. To minimize learner brain damage, we've taken care of them for you - just make sure you read it before you use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvBatch:\n",
    "    def __init__(self, n_envs = 10):\n",
    "        \"\"\" Creates n_envs environments and babysits them for ya' \"\"\"\n",
    "        self.envs = [make_env() for _ in range(n_envs)]\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\" Reset all games and return [n_envs, *obs_shape] observations \"\"\"\n",
    "        return np.array([env.reset() for env in self.envs])\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Send a vector[batch_size] of actions into respective environments\n",
    "        :returns: observations[n_envs, *obs_shape], rewards[n_envs], done[n_envs,], info[n_envs]\n",
    "        \"\"\"\n",
    "        results = [env.step(a) for env, a in zip(self.envs, actions)]\n",
    "        new_obs, rewards, done, infos = map(np.array, zip(*results))\n",
    "        \n",
    "        # reset environments automatically\n",
    "        for i in range(len(self.envs)):\n",
    "            if done[i]:\n",
    "                new_obs[i] = self.envs[i].reset()\n",
    "        \n",
    "        return new_obs, rewards, done, infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_batch = EnvBatch(10)\n",
    "\n",
    "batch_states = env_batch.reset()\n",
    "batch_actions = agent.choose_action(batch_states)\n",
    "batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n",
    "\n",
    "print(\"State shape:\", batch_states.shape)\n",
    "print(\"Actions:\", batch_actions)\n",
    "print(\"Rewards:\", batch_rewards)\n",
    "print(\"Done:\", batch_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(lr=1e-4, n_actions=n_actions, input_dims=obs_shape)\n",
    "\n",
    "state = env_batch.reset()\n",
    "action = agent.choose_action(state)\n",
    "\n",
    "next_state, reward, done, info = env_batch.step(action)\n",
    "\n",
    "l_act, l_crit, ent = compute_actor_critic_loss(agent, state, action, reward, next_state, done)\n",
    "\n",
    "assert abs(l_act) < 100 and abs(l_crit) < 100, \"losses seem abnormally large\"\n",
    "assert 0 <= ent.mean() <= np.log(n_actions), \"impossible entropy value, double-check the formula pls\"\n",
    "if ent.mean() < np.log(n_actions) / 2:\n",
    "    print(\"Entropy is too low for an untrained agent\")\n",
    "print(\"You just might be fine!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "Just the usual - play a bit, compute loss, follow the graidents, repeat a few million times.\n",
    "\n",
    "![Daniel San training the Karate Kid](https://media.giphy.com/media/W4uQMqlKVoiXK89T5j/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def ewma(x, span=100):\n",
    "    return pd.DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "env_batch = EnvBatch(10)\n",
    "batch_states = env_batch.reset()\n",
    "\n",
    "rewards_history = []\n",
    "entropy_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please pay extra attention to how we scale rewards in training. We do that for multiple reasons.\n",
    "\n",
    "1. All rewards are multiples of 100, and even an untrained agent can get a score of 800. Therefore, even in the beginning of training, the critic will have to predict pretty large numbers. Neural networks require extra tinkering to output large numbers reliably. In this case, the easiest workaround is just to scale back those numbers.\n",
    "2. We have already tweaked the hyperparameters (loss coefficients) to work well with this scaling.\n",
    "\n",
    "Please note, that we would not need to do this in plain REINFORCE without entropy regularization but with Adam optimizer.\n",
    "\n",
    "In REINFORCE, there is only actor and no critic. Without entropy regularization, actor loss is just policy gradient. It is proportional to rewards, but it only affects the scale of the gradient. However, Adam maintains a running average of the variance of the gradient for each parameter it optimizes, and normalizes the gradient by its variance in each optimization step. This will negate any scaling of the gradient.\n",
    "\n",
    "If your implementation works correctly, you can comment out the `batch_rewards = batch_rewards * 0.01` line, restart training, and see it explode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "with tqdm.trange(len(entropy_history), 100000) as progress_bar:\n",
    "    for i in progress_bar:\n",
    "        batch_actions = agent.choose_action(batch_states)\n",
    "        batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n",
    "\n",
    "        # Reward scaling. See above for explanation.\n",
    "        batch_rewards = batch_rewards * 0.01\n",
    "\n",
    "        agent_loss, critic_loss, entropy = compute_actor_critic_loss(\n",
    "            agent, batch_states, batch_actions, batch_rewards, batch_next_states, batch_done)\n",
    "        entropy_history.append(np.mean(entropy))\n",
    "\n",
    "        batch_states = batch_next_states\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            if i % 2500 == 0:\n",
    "                rewards_history.append(np.mean(evaluate(agent, env, n_games=3)))\n",
    "                if rewards_history[-1] >= 5000:\n",
    "                    print(\"Your agent has earned the yellow belt\")\n",
    "\n",
    "            clear_output(True)\n",
    "\n",
    "            plt.figure(figsize=[8, 4])\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(rewards_history, label='rewards')\n",
    "            plt.plot(ewma(np.array(rewards_history), span=10), marker='.', label='rewards ewma@10')\n",
    "            plt.title(\"Session rewards\")\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(entropy_history, label='entropy')\n",
    "            plt.plot(ewma(np.array(entropy_history), span=1000), marker='.', label='entropy ewma@1000')\n",
    "            plt.title(\"Policy entropy\")\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay, if it fluctuates ~~like crazy~~. It's also OK, if it doesn't increase substantially before 10-20k initial steps, and some people, who tried this assignment [told us](https://www.coursera.org/learn/practical-rl/discussions/all/threads/3OnFNVxEEemLZA644RFX2A) they didn't see improvements until approximately 60k steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, something wrong is happening.\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ â€” the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the likely culprit is:\n",
    "* Some bug in entropy computation. Remember, that it is $-\\sum p(a_i) \\cdot \\log p(a_i)$.\n",
    "* Your model architecture is broken in some way: for example, if you create layers in `Agent.symbolic_step()` rather than in `Agent.__init__()`, then effectively you will be training two separate agents: one for `logits, state_values` and another one for `next_logits, next_state_values`.\n",
    "* Your architecture is different from the one we suggest and it converges too quickly. Change your architecture or increase the entropy coefficient in actor loss. \n",
    "* Gradient explosion: just [clip gradients](https://stackoverflow.com/a/43486487) and maybe use a smaller network.\n",
    "* Us. Or TF developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you are likely to see some NaNs or insanely large numbers or zeros. Try to catch the moment, when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(make_env(), directory=\"videos\", force=True) as env_monitor:\n",
    "    final_rewards = evaluate(agent, env_monitor, n_games=3)\n",
    "\n",
    "print(\"Final mean reward:\", np.mean(final_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-2]))  # You can also try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't see videos above, just navigate to `./videos` and download `.mp4` files from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit import submit_kungfu\n",
    "env = make_env()\n",
    "submit_kungfu(agent, env, evaluate, 'your.email@example.com', 'YourAssignmentToken')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now what?\n",
    "Well, 5k reward is [just the beginning](https://www.buzzfeed.com/mattjayyoung/what-the-color-of-your-karate-belt-actually-means-lg3g). Can you get past 200? With recurrent neural network memory, chances are you can even beat 400!\n",
    "\n",
    "* Try n-step advantage and \"lambda\"-advantage (aka GAE) - see [this article](https://arxiv.org/abs/1506.02438)\n",
    " * This change should improve the early convergence a lot\n",
    "* Try recurrent a neural network \n",
    " * RNN memory will slow things down initially, but it will reach better final reward at this game\n",
    "* Implement asynchronuous version\n",
    " * Remember [A3C](https://arxiv.org/abs/1602.01783)? The first \"A\" stands for asynchronuous. It means there are several parallel actor-learners out there.\n",
    " * You can write the custom code for synchronization, but we recommend using [redis](https://redis.io/)\n",
    "   * You can store full parameter set in redis, along with any other metadate\n",
    "   * Here's a _quick_ way to (de)serialize parameters for redis\n",
    "   ```\n",
    "   import joblib\n",
    "   from six import BytesIO\n",
    "```\n",
    "```\n",
    "   def dumps(data):\n",
    "        \"converts whatever to string\"\n",
    "        s = BytesIO()\n",
    "        joblib.dump(data,s)\n",
    "        return s.getvalue()\n",
    "``` \n",
    "```\n",
    "    def loads(string):\n",
    "        \"converts string to whatever was dumps'ed in it\"\n",
    "        return joblib.load(BytesIO(string))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
