{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Kung-Fu with advantage actor-critic\n",
    "\n",
    "In this notebook you'll build a deep reinforcement learning agent for atari [KungFuMaster](https://gym.openai.com/envs/KungFuMaster-v0/) and train it with advantage actor-critic.\n",
    "\n",
    "![http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg](http://www.retroland.com/wp-content/uploads/2011/07/King-Fu-Master.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from IPython.core import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "#If you are running on a server, launch xvfb to record game videos\n",
    "#Please make sure you have xvfb installed\n",
    "import os\n",
    "if os.environ.get(\"DISPLAY\") is str and len(os.environ.get(\"DISPLAY\"))!=0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's take a look at the game itself:\n",
    "* Image resized to 42x42 and grayscale to run faster\n",
    "* Rewards divided by 100 'cuz they are all divisible by 100\n",
    "* Agent sees last 4 frames of game to account for object velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "# We scale rewards to avoid exploding gradients during optimization.\n",
    "reward_scale = 0.01\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(\n",
    "        env, height=42, width=42,\n",
    "        crop=lambda img: img[60:-30, 5:],\n",
    "        dim_order='tensorflow',\n",
    "        color=False, n_frames=4,\n",
    "        reward_scale=reward_scale)\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation (4-frame buffer)')\n",
    "plt.imshow(s.transpose([0,2,1]).reshape([42,-1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an agent\n",
    "\n",
    "We now have to build an agent for actor-critic training - a convolutional neural network that converts states into action probabilities $\\pi$ and state values $V$.\n",
    "\n",
    "Your assignment here is to build and apply a neural network - with any framework you want. \n",
    "\n",
    "For starters, we want you to implement this architecture:\n",
    "![https://s17.postimg.org/orswlfzcv/nnet_arch.png](https://s17.postimg.org/orswlfzcv/nnet_arch.png)\n",
    "\n",
    "After your agent gets mean reward above 50, we encourage you to experiment with model architecture to score even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Dense, Flatten\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name, state_shape, n_actions, reuse=False):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "        \n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "            \n",
    "            # Prepare neural network architecture\n",
    "            ### Your code here: prepare any necessary layers, variables, etc.\n",
    "            \n",
    "            # prepare a graph for agent step\n",
    "            self.state_t = tf.placeholder('float32', [None,] + list(state_shape))\n",
    "            self.agent_outputs = self.symbolic_step(self.state_t)\n",
    "        \n",
    "    def symbolic_step(self, state_t):\n",
    "        \"\"\"Takes agent's previous step and observation, returns next state and whatever it needs to learn (tf tensors)\"\"\"\n",
    "        \n",
    "        # Apply neural network\n",
    "        ### Your code here: apply agent's neural network to get policy logits and state values.\n",
    "        \n",
    "        logits = <logits go here>\n",
    "        state_value = <state values go here>\n",
    "        \n",
    "        assert tf.is_numeric_tensor(state_value) and state_value.shape.ndims == 1, \\\n",
    "            \"please return 1D tf tensor of state values [you got %s]\" % repr(state_value)\n",
    "        assert tf.is_numeric_tensor(logits) and logits.shape.ndims == 2, \\\n",
    "            \"please return 2d tf tensor of logits [you got %s]\" % repr(logits)\n",
    "        # hint: if you triggered state_values assert with your shape being [None, 1], \n",
    "        # just select [:, 0]-th element of state values as new state values\n",
    "        \n",
    "        return (logits, state_value)\n",
    "    \n",
    "    def step(self, state_t):\n",
    "        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n",
    "        sess = tf.get_default_session()\n",
    "        return sess.run(self.agent_outputs, {self.state_t: state_t})\n",
    "    \n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        policy = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "        return np.array([np.random.choice(len(p), p=p) for p in policy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\"agent\", obs_shape, n_actions)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [env.reset()]\n",
    "logits, value = agent.step(state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an a game from start till done, returns per-game rewards \"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        state = env.reset()\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = agent.sample_actions(agent.step([state]))[0]\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done: break\n",
    "\n",
    "        # We rescale the reward back to ensure compatibility\n",
    "        # with other evaluations.\n",
    "        game_rewards.append(total_reward / reward_scale)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "env_monitor.close()\n",
    "print (rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "import os\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = [s for s in os.listdir(\"./kungfu_videos/\") if s.endswith(\".mp4\")]\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\" + video_names[-1]))  #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "![img](https://s7.postimg.org/4y36s2b2z/env_pool.png)\n",
    "\n",
    "To make actor-critic training more stable, we shall play several games in parallel. This means ya'll have to initialize several parallel gym envs, send agent's actions there and .reset() each env if it becomes terminated. To minimize learner brain damage, we've taken care of them for ya - just make sure you read it before you use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvBatch:\n",
    "    def __init__(self, n_envs = 10):\n",
    "        \"\"\" Creates n_envs environments and babysits them for ya' \"\"\"\n",
    "        self.envs = [make_env() for _ in range(n_envs)]\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\" Reset all games and return [n_envs, *obs_shape] observations \"\"\"\n",
    "        return np.array([env.reset() for env in self.envs])\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Send a vector[batch_size] of actions into respective environments\n",
    "        :returns: observations[n_envs, *obs_shape], rewards[n_envs], done[n_envs,], info[n_envs]\n",
    "        \"\"\"\n",
    "        results = [env.step(a) for env, a in zip(self.envs, actions)]\n",
    "        new_obs, rewards, done, infos = map(np.array, zip(*results))\n",
    "        \n",
    "        # reset environments automatically\n",
    "        for i in range(len(self.envs)):\n",
    "            if done[i]:\n",
    "                new_obs[i] = self.envs[i].reset()\n",
    "        \n",
    "        return new_obs, rewards, done, infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_batch = EnvBatch(10)\n",
    "\n",
    "batch_states = env_batch.reset()\n",
    "\n",
    "batch_actions = agent.sample_actions(agent.step(batch_states))\n",
    "\n",
    "batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n",
    "\n",
    "print(\"State shape:\", batch_states.shape)\n",
    "print(\"Actions:\", batch_actions[:3])\n",
    "print(\"Rewards:\", batch_rewards[:3])\n",
    "print(\"Done:\", batch_done[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic\n",
    "\n",
    "Here we define a loss functions and learning algorithms as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These placeholders mean exactly the same as in \"Let's try it out\" section above\n",
    "states_ph = tf.placeholder('float32', [None,] + list(obs_shape))    \n",
    "next_states_ph = tf.placeholder('float32', [None,] + list(obs_shape))\n",
    "actions_ph = tf.placeholder('int32', (None,))\n",
    "rewards_ph = tf.placeholder('float32', (None,))\n",
    "is_done_ph = tf.placeholder('float32', (None,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits[n_envs, n_actions] and state_values[n_envs, n_actions]\n",
    "logits, state_values = agent.symbolic_step(states_ph)\n",
    "next_logits, next_state_values = agent.symbolic_step(next_states_ph)\n",
    "next_state_values = next_state_values * (1 - is_done_ph)\n",
    "\n",
    "# probabilities and log-probabilities for all actions\n",
    "probs = tf.nn.softmax(logits)            # [n_envs, n_actions]\n",
    "logprobs = tf.nn.log_softmax(logits)     # [n_envs, n_actions]\n",
    "\n",
    "# log-probabilities only for agent's chosen actions\n",
    "logp_actions = tf.reduce_sum(logprobs * tf.one_hot(actions_ph, n_actions), axis=-1) # [n_envs,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# compute advantage using rewards_ph, state_values and next_state_values\n",
    "gamma = 0.99\n",
    "advantage = <YOUR CODE>\n",
    "\n",
    "assert advantage.shape.ndims == 1, \"please compute advantage for each sample, vector of shape [n_envs,]\"\n",
    "\n",
    "# compute policy entropy given logits_seq. Mind the \"-\" sign!\n",
    "entropy = <YOUR CODE>\n",
    "\n",
    "assert entropy.shape.ndims == 1, \"please compute pointwise entropy vector of shape [n_envs,] \"\n",
    "\n",
    "\n",
    "\n",
    "actor_loss =  - tf.reduce_mean(logp_actions * tf.stop_gradient(advantage)) - 0.001 * tf.reduce_mean(entropy)\n",
    "\n",
    "# compute target state values using temporal difference formula. Use rewards_ph and next_step_values\n",
    "target_state_values = <YOUR CODE>\n",
    "\n",
    "critic_loss = tf.reduce_mean((state_values - tf.stop_gradient(target_state_values))**2 )\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(actor_loss + critic_loss)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks to catch some errors. Specific to KungFuMaster in assignment's default setup.\n",
    "l_act, l_crit, adv, ent = sess.run([actor_loss, critic_loss, advantage, entropy], feed_dict = {\n",
    "        states_ph: batch_states,\n",
    "        actions_ph: batch_actions,\n",
    "        next_states_ph: batch_states,\n",
    "        rewards_ph: batch_rewards,\n",
    "        is_done_ph: batch_done,\n",
    "    })\n",
    "\n",
    "assert abs(l_act) < 100 and abs(l_crit) < 100, \"losses seem abnormally large\"\n",
    "assert 0 <= ent.mean() <= np.log(n_actions), \"impossible entropy value, double-check the formula pls\"\n",
    "if ent.mean() < np.log(n_actions) / 2: print(\"Entropy is too low for untrained agent\")\n",
    "print(\"You just might be fine!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "Just the usual - play a bit, compute loss, follow the graidents, repeat a few million times.\n",
    "![img](http://images6.fanpop.com/image/photos/38900000/Daniel-san-training-the-karate-kid-38947361-499-288.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import DataFrame\n",
    "ewma = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
    "\n",
    "env_batch = EnvBatch(10)\n",
    "batch_states = env_batch.reset()\n",
    "\n",
    "rewards_history = []\n",
    "entropy_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(100000):\n",
    "    batch_actions = agent.sample_actions(agent.step(batch_states))\n",
    "    batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n",
    "\n",
    "    feed_dict = {\n",
    "        states_ph: batch_states,\n",
    "        actions_ph: batch_actions,\n",
    "        next_states_ph: batch_next_states,\n",
    "        rewards_ph: batch_rewards,\n",
    "        is_done_ph: batch_done,\n",
    "    }\n",
    "    batch_states = batch_next_states\n",
    "\n",
    "    _, ent_t = sess.run([train_step, entropy], feed_dict)\n",
    "    entropy_history.append(np.mean(ent_t))\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        if i % 2500 == 0:\n",
    "            rewards_history.append(np.mean(evaluate(agent, env, n_games=3)))\n",
    "            if rewards_history[-1] >= 50:\n",
    "                print(\"Your agent has earned the yellow belt\" % color)\n",
    "\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=[8, 4])\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(ewma(np.array(rewards_history), span=10), marker='.', label='rewards ewma@10')\n",
    "        plt.title(\"Session rewards\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(entropy_history, label='entropy')\n",
    "        plt.plot(ewma(np.array(entropy_history), span=1000), label='entropy ewma@1000')\n",
    "        plt.title(\"Policy entropy\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it reward doesn't increase substantially before some 10k initial steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ - the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Your agent architecture converges too fast. Increase entropy coefficient in actor loss. \n",
    "* Gradient explosion - just [clip gradients](https://stackoverflow.com/a/43486487) and maybe use a smaller network\n",
    "* Us. Or TF developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward:\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./kungfu_videos/\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\" + video_names[-2]))  # try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't see videos, just navigate to ./kungfu_videos and download .mp4 files from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit import submit_kungfu\n",
    "env = make_env()\n",
    "submit_kungfu(agent, env, evaluate, <EMAIL>, <TOKEN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now what?\n",
    "Well, 5k reward is [just the beginning](https://www.buzzfeed.com/mattjayyoung/what-the-color-of-your-karate-belt-actually-means-lg3g). Can you get past 200? With recurrent neural network memory, chances are you can even beat 400!\n",
    "\n",
    "* Try n-step advantage and \"lambda\"-advantage (aka GAE) - see [this article](https://arxiv.org/abs/1506.02438)\n",
    " * This change should improve early convergence a lot\n",
    "* Try recurrent neural network \n",
    " * RNN memory will slow things down initially, but in will reach better final reward at this game\n",
    "* Implement asynchronuous version\n",
    " * Remember [A3C](https://arxiv.org/abs/1602.01783)? The first \"A\" stands for asynchronuous. It means there are several parallel actor-learners out there.\n",
    " * You can write custom code for synchronization, but we recommend using [redis](https://redis.io/)\n",
    "   * You can store full parameter set in redis, along with any other metadate\n",
    "   * Here's a _quick_ way to (de)serialize parameters for redis\n",
    "   ```\n",
    "   import joblib\n",
    "   from six import BytesIO\n",
    "```\n",
    "```\n",
    "   def dumps(data):\n",
    "        \"converts whatever to string\"\n",
    "        s = BytesIO()\n",
    "        joblib.dump(data,s)\n",
    "        return s.getvalue()\n",
    "``` \n",
    "```\n",
    "    def loads(string):\n",
    "        \"converts string to whatever was dumps'ed in it\"\n",
    "        return joblib.load(BytesIO(string))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
