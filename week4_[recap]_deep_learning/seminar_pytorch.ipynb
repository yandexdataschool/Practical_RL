{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, pytorch\n",
    "\n",
    "![img](https://s1.postimg.org/6fl45xnvnj/pytorch-logo-dark.png)\n",
    "\n",
    "Hello, github dweller!\n",
    "\n",
    "__This notebook__ will teach you to use pytorch low-level core. You can install it [here](http://pytorch.org/). For high-level interface see the next notebook.\n",
    "\n",
    "__Pytorch feels__ differently than tensorflow/theano in almost every level. TensorFlow makes your code live in two \"worlds\" simultaneously:  symbolic graphs and actual tensors. First you declare a symbolic \"recipe\" of how to get from inputs to outputs, then feed it with actual minibatches of data.  In pytorch, __there's only one world__: all tensors have a numeric value.\n",
    "\n",
    "You compute outputs on the fly without pre-declaring anything. The code looks exactly as in pure numpy with one exception: pytorch computes gradients for you. And can run stuff on GPU. And has a number of pre-implemented building blocks for your neural nets. [And a few more things.](https://medium.com/towards-data-science/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b)\n",
    "\n",
    "And now we finally shut up and let pytorch do the talking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "add 5 :\n",
      "[[ 5  6  7  8]\n",
      " [ 9 10 11 12]\n",
      " [13 14 15 16]\n",
      " [17 18 19 20]]\n",
      "X*X^T  :\n",
      " [[ 14  38  62  86]\n",
      " [ 38 126 214 302]\n",
      " [ 62 214 366 518]\n",
      " [ 86 302 518 734]]\n",
      "mean over cols :\n",
      "[  1.5   5.5   9.5  13.5]\n",
      "cumsum of cols :\n",
      "[[ 0  1  2  3]\n",
      " [ 4  6  8 10]\n",
      " [12 15 18 21]\n",
      " [24 28 32 36]]\n"
     ]
    }
   ],
   "source": [
    "# numpy world\n",
    "\n",
    "x = np.arange(16).reshape(4,4)\n",
    "\n",
    "print (\"X :\\n%s\"%x)\n",
    "print (\"add 5 :\\n%s\"%(x + 5))\n",
    "print (\"X*X^T  :\\n\",np.dot(x,x.T))\n",
    "print (\"mean over cols :\\n%s\"%(x.mean(axis=-1)))\n",
    "print (\"cumsum of cols :\\n%s\"%(np.cumsum(x,axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :\n",
      "\n",
      "  0   1   2   3\n",
      "  4   5   6   7\n",
      "  8   9  10  11\n",
      " 12  13  14  15\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "add 5 :\n",
      " \n",
      "  5   6   7   8\n",
      "  9  10  11  12\n",
      " 13  14  15  16\n",
      " 17  18  19  20\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "X*X^T  :\n",
      " \n",
      "  14   38   62   86\n",
      "  38  126  214  302\n",
      "  62  214  366  518\n",
      "  86  302  518  734\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "mean over cols :\n",
      " \n",
      "  1.5000\n",
      "  5.5000\n",
      "  9.5000\n",
      " 13.5000\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "cumsum of cols :\n",
      " \n",
      "  0   1   2   3\n",
      "  4   6   8  10\n",
      " 12  15  18  21\n",
      " 24  28  32  36\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pytorch world\n",
    "\n",
    "x = np.arange(16).reshape(4,4)\n",
    "\n",
    "x = torch.from_numpy(x).type(torch.FloatTensor) #or torch.arange(0,16).view(4,4)\n",
    "\n",
    "print (\"X :\\n%s\"%x)\n",
    "print (\"add 5 :\\n%s\"%(x+5))\n",
    "print (\"X*X^T  :\\n\",torch.matmul(x,x.transpose(1,0)))\n",
    "print (\"mean over cols :\\n\",torch.mean(x,dim=-1))\n",
    "print (\"cumsum of cols :\\n\",torch.cumsum(x,dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy and Pytorch\n",
    "\n",
    "As you can notice, pytorch allows you to hack stuff much the same way you did with numpy. No graph declaration, no placeholders, no sessions. This means that you can _see the numeric value of any tensor at any moment of time_. Debugging such code can be done with by printing tensors or using any debug tool you want (e.g. [gdb](https://wiki.python.org/moin/DebuggingWithGdb)).\n",
    "\n",
    "You could also notice the a few new method names and a different API. So no, there's no compatibility with numpy [yet](https://github.com/pytorch/pytorch/issues/2228) and yes, you'll have to memorize all the names again. Cheers!\n",
    "\n",
    "![img](http://i0.kym-cdn.com/entries/icons/original/000/017/886/download.jpg)\n",
    "\n",
    "For example, \n",
    "* If something takes a list/tuple of axes in numpy, you can expect it to take *args in pytorch\n",
    " * `x.reshape([1,2,8]) -> x.view(1,2,8)`\n",
    "* You should swap _axis_ for _dim_ in operations like mean or cumsum\n",
    " * `x.sum(axis=-1) -> x.sum(dim=-1)`\n",
    "* most mathematical operations are the same, but types an shaping is different\n",
    " * `x.astype('int64') -> x.type(torch.LongTensor)`\n",
    "\n",
    "To help you acclimatize, there's a [table](https://github.com/torch/torch7/wiki/Torch-for-Numpy-users) covering most new things. There's also a neat [documentation page](http://pytorch.org/docs/master/).\n",
    "\n",
    "Finally, if you're stuck with a technical problem, we recommend searching [pytorch forumns](https://discuss.pytorch.org/). Or just googling, which usually works just as efficiently. \n",
    "\n",
    "If you feel like you almost give up, remember two things: __GPU__ an __free gradients__. Besides you can always jump back to numpy with x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0,2*np.pi,16)\n",
    "\n",
    "#Mini-task: compute a vector of sin^2(x) + cos^2(x)\n",
    "out = <your code here>\n",
    "\n",
    "print(out.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Tensor vs Variable\n",
    "![img](https://s1.postimg.org/1i8tinij33/torch_tensor_variable.png)\n",
    "\n",
    "`    ` Tensor and Variable Weasley were identical and mischievous twin classes living in pytorch. Brilliant pranksters, they make sure your code never runs successfully from the first attempt.\n",
    "\n",
    "`    ` Seriously though, pytorch suddenly has 2 main abstractions:\n",
    "* Tensor, that we've just used\n",
    "* Variable, which extends tensor and allows for automatic gradients\n",
    "\n",
    "Ideally, you could wrap everything into Variable from the get-go an forget about Tensor. Hopefully you will once be able to do so [upvote [this](https://github.com/pytorch/pytorch/issues/2228)]. Right now you can't.\n",
    "\n",
    "\n",
    "Some operations only work on tensors (e.g. bitwise &|~), some functions require variables (torch.nn.functional.whatever).\n",
    "\n",
    "The good news is that you can always swap between the two seamlessly\n",
    "* tensor to variable: `Variable(x)`\n",
    "* variable to tensor: `x.data`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "x = torch.arange(0,16).view(4,4).type(torch.IntTensor)\n",
    "x_var = Variable(x)\n",
    "\n",
    "print (\"Logical or (tensors):\\n\",(x==1) | (x%3==0))\n",
    "\n",
    "#will not work\n",
    "try:\n",
    "    print (\"Logical or (variables):\")\n",
    "    print ((x_var==1) | (x_var%3==0))\n",
    "except Exception,e:\n",
    "    print (e)\n",
    "#instead: (x_var.data==1) | (x_var.data%3==0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works:\n",
    "sequence = torch.randn(1,8,10)\n",
    "filters = torch.randn(2,8,3)\n",
    "\n",
    "#will work:\n",
    "print(\"conv1d (variables):\\n\",torch.nn.functional.conv1d(Variable(sequence),Variable(filters)))\n",
    "\n",
    "#will not work:\n",
    "try:\n",
    "    print(\"conv1d (tensors):\")\n",
    "    print(torch.nn.functional.conv1d(sequence,filters))\n",
    "except Exception,e:\n",
    "    print (e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task I: the game of life\n",
    "\n",
    "Now it's time for you to make something more challenging. We'll implement Conway's [Game of Life](http://web.stanford.edu/~cdebs/GameOfLife/) in _pure pytorch_. \n",
    "\n",
    "While this is still a toy task, implementing game of life this way has one cool benefit: __you'll be able to run it on GPU! __ Indeed, what could be a better use of your gpu than simulating game of life on 1M/1M grids?\n",
    "\n",
    "![img](https://cdn.tutsplus.com/gamedev/authors/legacy/Stephane%20Beniak/2012/09/11/Preview_Image.png)\n",
    "If you've skipped the url above out of sloth, here's the game of life:\n",
    "* You have a 2D grid of cells, where each cell is \"alive\"(1) or \"dead\"(0)\n",
    "* Any living cell that has 2 or 3 neighbors survives, else it dies [0,1 or 4+ neighbors]\n",
    "* Any cell with exactly 3 neighbors becomes alive (if it was dead)\n",
    "\n",
    "For this task, you are given a reference numpy implementation that you must convert to pytorch.\n",
    "_[numpy code inspired by: https://github.com/rougier/numpy-100]_\n",
    "\n",
    "\n",
    "__Note:__ You can find convolution in `torch.nn.functional.conv2d(Z,filters)`. Note that it has a different input format. \n",
    "\n",
    "It's also convenient to `import torch.nn.functional as F`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import convolve2d\n",
    "def np_update(Z):\n",
    "    # Count neighbours with convolution\n",
    "    filters = np.array([[1,1,1],\n",
    "                        [1,0,1],\n",
    "                        [1,1,1]])\n",
    "    \n",
    "    N = convolve2d(Z,filters,mode='same')\n",
    "    \n",
    "    # Apply rules\n",
    "    birth = (N==3) & (Z==0)\n",
    "    survive = ((N==2) | (N==3)) & (Z==1)\n",
    "    \n",
    "    Z[:] = birth | survive\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def torch_update(Z):\n",
    "    \"\"\"\n",
    "    Implement an update function that does to Z exactly the same as np_update.\n",
    "    :param Z: torch.FloatTensor of shape [height,width] containing 0s(dead) an 1s(alive)\n",
    "    :returns: torch.FloatTensor Z after updates.\n",
    "    \n",
    "    You can opt to create new tensor or change Z inplace.\n",
    "    \"\"\"\n",
    "    \n",
    "    #<Your code here!>\n",
    "    \n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#initial frame\n",
    "Z_numpy = np.random.choice([0,1],p=(0.5,0.5),size=(100,100))\n",
    "Z = torch.from_numpy(Z_numpy).type(torch.FloatTensor)\n",
    "\n",
    "#your debug polygon :)\n",
    "Z_new = torch_update(Z.copy())\n",
    "\n",
    "#tests\n",
    "Z_reference = np_update(Z_numpy.copy())\n",
    "assert np.all(Z_new.numpy() == Z_reference), \"your pytorch implementation doesn't match np_update. Look into Z and np_update(ZZ) to investigate.\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "\n",
    "#initialize game field\n",
    "Z = np.random.choice([0,1],size=(100,100))\n",
    "Z = torch.from_numpy(Z).type(torch.FloatTensor)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fig.show()\n",
    "\n",
    "for _ in range(100):\n",
    "    \n",
    "    #update\n",
    "    Z = torch_update(Z)\n",
    "    \n",
    "    #re-draw image\n",
    "    ax.clear()\n",
    "    ax.imshow(Z.numpy(),cmap='gray')\n",
    "    fig.canvas.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some fun setups with GoL for your amusement\n",
    "\n",
    "#parallel stripes\n",
    "Z = np.arange(100)%2 + np.zeros([100,100])\n",
    "#with a small imperfection\n",
    "Z[48:52,50]=1\n",
    "\n",
    "Z = torch.from_numpy(Z).type(torch.FloatTensor)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fig.show()\n",
    "\n",
    "for _ in range(100):\n",
    "    Z = torch_update(Z)\n",
    "    ax.clear()\n",
    "    ax.imshow(Z.numpy(),cmap='gray')\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More fun with Game of Life: [video](https://www.youtube.com/watch?v=C2vgICfQawE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic gradients\n",
    "\n",
    "Like most deep learning frameworks, pytorch can be persuaded to compute derivatives for you.\n",
    "\n",
    "The general pipeline looks like this:\n",
    "* You create ```a = Variable(...,requires_grad=True)```\n",
    "* You define some differentiable `loss = whatever(a)`\n",
    "* `loss.backward()`\n",
    "* Gradients are available as ```a.grads```\n",
    "\n",
    "Unlike theano or tensorflow, the loss function is defined dynamically on each minibatch, letting you change loss function on each step.\n",
    "\n",
    "Another difference is that gradients are always stored in `.grad` of a variable. If you compute gradient from multiple losses, this will cause gradients to add up at variables, therefore it's useful to __zero the gradients__ between computations.\n",
    "\n",
    "\n",
    "__Here's an example:__ fitting a linear regression the hard way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "x,y = load_boston(return_X_y=True)\n",
    "\n",
    "#select one column for simplicity. \n",
    "x = x[:,-1] / x[:,-1].std()\n",
    "y = y / y.std()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model variables\n",
    "w = Variable(torch.zeros(1),requires_grad=True)\n",
    "b = Variable(torch.zeros(1),requires_grad=True)\n",
    "\n",
    "# data variables\n",
    "x = Variable(torch.from_numpy(x)).type(torch.FloatTensor)\n",
    "y = Variable(torch.from_numpy(y)).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try out gradients\n",
    "y_pred = w * x  + b\n",
    "loss = torch.mean((y_pred - y)**2)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "#now w.grad is a Variable containing gradient of L w.r.t. w\n",
    "\n",
    "print(\"dL/dw = \\n\",w.grad)\n",
    "print(\"dL/db = \\n\",b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use it to __perform gradient descent__.\n",
    "\n",
    "This is as low-level a code as it can get for pytorch offers many pre-implemented learning methods. We'll delve into those methods in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    #compute loss\n",
    "    y_pred = w * x  + b\n",
    "    loss = torch.mean((y_pred - y)**2)\n",
    "    \n",
    "    #gradient descent\n",
    "    loss.backward()\n",
    "\n",
    "    w.data -= 0.1*w.grad.data\n",
    "    b.data -= 0.1*b.grad.data\n",
    "    \n",
    "    #zero gradients\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    \n",
    "    #the rest of code is just bells and whistles\n",
    "    if (i+1)%5==0:\n",
    "        #draw linear regression prediction vs data\n",
    "        clear_output(True)\n",
    "        plt.axhline(0, color='gray')\n",
    "        plt.axvline(0, color='gray')\n",
    "        plt.scatter(x.data.numpy(),y.data.numpy())\n",
    "        plt.plot(x.data.numpy(),y_pred.data.numpy(),color='orange')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"loss = \", loss.data.numpy()[0])\n",
    "        if loss.data.numpy()[0] < 0.5:\n",
    "            print(\"Done!\")\n",
    "            break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same, but simpler with `torch.optim`\n",
    "\n",
    "To avoid keeping track of every single variable, you could instead use a pre-defined optimizer from torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "x,y = load_boston(return_X_y=True)\n",
    "x = x[:,-1] / x[:,-1].std()\n",
    "y = y / y.std()\n",
    "\n",
    "# data variables\n",
    "x = Variable(torch.from_numpy(x)).type(torch.FloatTensor)\n",
    "y = Variable(torch.from_numpy(y)).type(torch.FloatTensor)\n",
    "\n",
    "# model variables\n",
    "w = Variable(torch.zeros(1),requires_grad=True)\n",
    "b = Variable(torch.zeros(1),requires_grad=True)\n",
    "\n",
    "# define optimizer\n",
    "opt = torch.optim.RMSprop([w,b],lr=0.1)\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    #compute loss\n",
    "    y_pred = w * x  + b\n",
    "    loss = torch.mean((y_pred - y)**2)\n",
    "    \n",
    "    #gradient descent\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    #the rest of code is just bells and whistles\n",
    "    if (i+1)%5==0:\n",
    "        #draw linear regression prediction vs data\n",
    "        clear_output(True)\n",
    "        plt.axhline(0, color='gray')\n",
    "        plt.axvline(0, color='gray')\n",
    "        plt.scatter(x.data.numpy(),y.data.numpy())\n",
    "        plt.plot(x.data.numpy(),y_pred.data.numpy(),color='orange')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"loss = \", loss.data.numpy()[0])\n",
    "        if loss.data.numpy()[0] < 0.5:\n",
    "            print(\"Done!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task II: A vs B problem\n",
    "\n",
    "Once you've learned how to perform the gradient descent with PyTorch, it's time to put this knowledge to some use. We'll write logistic regression to classify letters.\n",
    "\n",
    "We want you to write your very own logistic regression to classify NotMnist characters. For this simplified task, we'll only consider two classes: __\"A\"__ vs __\"B\"__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notmnist small\n",
    "!curl http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz > notMNIST_small.tar.gz\n",
    "!tar -zxvf notMNIST_small.tar.gz > untar_notmnist.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.misc import imread,imresize\n",
    "from glob import glob\n",
    "def load_notmnist(path='./notMNIST_small',letters='ABCDEFGHIJ',img_shape = (28,28)):\n",
    "    data,labels = [],[]\n",
    "    print(\"Parsing...\")\n",
    "    for img_path in glob(os.path.join(path,'*/*')):\n",
    "        class_i = img_path.split('/')[-2]\n",
    "        if class_i not in letters: continue\n",
    "        try:\n",
    "            data.append(imresize(imread(img_path),img_shape))\n",
    "            labels.append(class_i,)\n",
    "        except:\n",
    "            print(\"found broken img: %s [it's ok if <10 images are broken]\"%img_path)\n",
    "        \n",
    "    data = np.stack(data)[:,None]\n",
    "    data = data.astype(np.float32)\n",
    "    data = (data - np.mean(data))/np.std(data)\n",
    "\n",
    "    #convert classes to ints\n",
    "    letter_to_i = {l:i for i,l in enumerate(letters)}\n",
    "    labels = np.array(list(map(letter_to_i.get, labels)))\n",
    "    return data,labels\n",
    "\n",
    "data,labels = load_notmnist(letters='AB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing...\n",
      "found broken img: ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png [it's ok if <10 images are broken]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f49c53de750>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADHCAYAAAAJSqg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49c9b09690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(data[0].reshape([28,28]))\n",
    "plt.title(str(labels[0]))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(data[-1].reshape([28,28]))\n",
    "plt.title(str(labels[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size = 3370, test_size = 375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(data,labels,test_size=0.1)\n",
    "\n",
    "print(\"Train size = %i, test_size = %i\"%(len(X_train),len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a binary classification problem, so please train a __Logistic Regression with sigmoid__, not softmax.\n",
    "\n",
    "You should use binary crossentropy (aka logloss) as your objective:\n",
    "$$ L = - {1 \\over N} \\underset{X_i,y_i} \\sum y_i \\cdot log P(y_i | X_i) + (1-y_i) \\cdot log (1-P(y_i | X_i))$$\n",
    "\n",
    "Where probabilities are outputs of a logistic regression model:\n",
    "\n",
    "$$P(y_i | X_i) = \\sigma(\\vec w \\cdot X_i + b) ={ 1 \\over {1+e^{- [\\vec w \\cdot X_i + b]}} }$$\n",
    "\n",
    "$$[\\vec w \\cdot X_i \\space denotes \\space dot \\space product]$$\n",
    "\n",
    "Your model therefore has two trainable variables:\n",
    "* $\\vec w$ - a vector with as many elements as there are features (pixels)\n",
    "* $b$ - a single number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create data variables\n",
    "# YOUR CODE HERE!\n",
    "\n",
    "# Create model variables\n",
    "# YOUR CODE HERE!\n",
    "\n",
    "# [optional] create optimizer from pytorch.optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    \n",
    "    # predict P(y_train|X_train)\n",
    "    \n",
    "    # compute loss\n",
    "    \n",
    "    # gradient descent\n",
    "    \n",
    "    # zero-out gradients\n",
    "    \n",
    "    # optional: plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#predict P(y|x) for X_test as a numpy array\n",
    "#hint: use .numpy() to get cast torch tensor into numpy array\n",
    "predicted_probabilities = <YOUR CODE HERE>\n",
    "\n",
    "accuracy = accuracy_score(y_test,predicted_probabilities)\n",
    "print(\"Accuracy: %.5f\"%)\n",
    "if accuracy > 0.95:\n",
    "    print(\"Well done! You've beaten the assignment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging tips:\n",
    "* initialize w and b either with zeros or with __small__ numbers. Sampling w ~ N(0,1) isn't small enough.\n",
    "* make sure your model predicts probabilities correctly. Just print them and see what's inside.\n",
    "* don't forget _minus_ sign in the loss function! It's a mistake 99% ppl do.\n",
    "* In general, pytorch's error messages are quite helpful, read 'em before you google 'em.\n",
    "* if you see nan/inf, print what happens at each iteration to find our where exactly it occurs.\n",
    "* Don't forget to zero-out gradients after each step. Srsly:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "<img src=\"http://download.gamezone.com/uploads/image/data/1190338/article_post_width_a88.jpg\" width=360>\n",
    "\n",
    "### Going deeper\n",
    "Your ultimate task for this week is to build your first neural network [almost] from scratch and pure torch.\n",
    "\n",
    "This time you will solve the same digit recognition problem, but at a greater scale\n",
    "* 10 different letters\n",
    "* 20k samples\n",
    "\n",
    "With 10 classes you will need to use softmax instead of sigmoid and a multiclass version log loss (see [here](https://www.kaggle.com/wiki/LogLoss)). You can also expect your logistic regression to be less accurate.\n",
    "\n",
    "Note that you are not required to build 152-layer monsters here. A 2-layer (one hidden, one output) NN should already give you an edge over logistic regression.\n",
    "\n",
    "__[bonus score]__\n",
    "If you've already beaten logistic regression with a two-layer net, but enthusiasm still ain't gone, you can try improving the test accuracy even further! It should be possible to reach 90% without convnets.\n",
    "\n",
    "__SPOILER!__\n",
    "At the end of the notebook you will find a few tips and frequently made mistakes. If you feel enough might to shoot yourself in the foot without external assistance, we encourage you to do so, but if you encounter any unsurpassable issues, please do look there before mailing us.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing...\n",
      "found broken img: ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png [it's ok if <10 images are broken]\n",
      "found broken img: ./notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png [it's ok if <10 images are broken]\n"
     ]
    }
   ],
   "source": [
    "data,labels = load_notmnist(letters='ABCDEFGHIJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#< a whole lot of your code > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# SPOILERS!\n",
    "\n",
    "Recommended pipeline\n",
    "\n",
    "* Adapt logistic regression from previous assignment to classify some number against others (e.g. zero vs nonzero)\n",
    "* Generalize it to multiclass logistic regression.\n",
    "  - Either try to remember lecture 0 or google it.\n",
    "  - Instead of weight vector you'll have to use matrix (feature_id x class_id)\n",
    "  - softmax (exp over sum of exps) can implemented manually or as T.nnet.softmax (stable)\n",
    "  - probably better to use STOCHASTIC gradient descent (minibatch) for greater speed\n",
    "    - you can also try momentum/rmsprop/adawhatever\n",
    "    - in which case sample should probably be shuffled (or use random subsamples on each iteration)\n",
    "* Add a hidden layer. Now your logistic regression uses hidden neurons instead of inputs.\n",
    "  - Hidden layer uses the same math as output layer (ex-logistic regression), but uses some nonlinearity (e.g. sigmoid) instead of softmax\n",
    "  - You need to train both layers, not just output layer :)\n",
    "  - __Do not initialize weights with zeros__ (due to symmetry effects). A gaussian noize with small variance will do.\n",
    "  - 50 hidden neurons and a sigmoid nonlinearity will do for a start. Many ways to improve. \n",
    "  - In ideal casae this totals to 2 .dot's, 1 softmax and 1 sigmoid\n",
    "  - __make sure this neural network works better than logistic regression__\n",
    "  \n",
    "* Now's the time to try improving the network. Consider layers (size, neuron count),  nonlinearities, optimization methods, initialization - whatever you want, but please avoid convolutions for now.\n",
    "  \n",
    "\n",
    "### Coming next\n",
    "\n",
    "So far we have only used the low level interface of PyTorch. In the next notebook, we'll cover how to train models more conveniently with high level PyTorch API: namely, the __torch.nn__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More links:\n",
    "* Using torch on GPU and multi-GPU - [link](http://pytorch.org/docs/master/notes/cuda.html)\n",
    "* More tutorials on pytorch - [link](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "* Pytorch examples - a repo that implements many cool DL models in pytorch - [link](https://github.com/pytorch/examples)\n",
    "* Practical pytorch - a repo that implements some... other cool DL models... yes, in pytorch - [link](https://github.com/spro/practical-pytorch)\n",
    "* And some more - [link](https://www.reddit.com/r/pytorch/comments/6z0yeo/pytorch_and_pytorch_tricks_for_kaggle/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
