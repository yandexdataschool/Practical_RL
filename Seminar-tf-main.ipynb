{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salary prediction via Deep NLP methods\n",
    "A recent Kaggle competiton (merely 5 years) propose you to find out who is the most well-paid professional\n",
    "\n",
    "We are gonna solve regression task.\n",
    "\n",
    "Competition is available here: https://www.kaggle.com/c/job-salary-prediction\n",
    "\n",
    "![Hobby](https://imgs.xkcd.com/comics/extrapolating.png)\n",
    "\n",
    "In this notebook you will learn\n",
    " - Data preprocessing for NLP or the most annoying part of data scientist's job\n",
    " - Convolutional Neural Networks for texts\n",
    " - Constructing you NN with scripting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Dataset is consists of job data. Most of it is an unstructured text. You should predict annual salary\n",
    "\n",
    "Download here: https://yadi.sk/d/vVEOWPFY3NruT7 or from competition\n",
    "\n",
    "## Main fields\n",
    "\n",
    "Title - A freetext field supplied to us by the job advertiser as the Title of the job ad.  Normally this is a summary of the job title or role.\n",
    "\n",
    "FullDescription - The full text of the job ad as provided by the job advertiser.  Where you see ***s, we have stripped values from the description in order to ensure that no salary information appears within the descriptions.  There may be some collateral damage here where we have also removed other numerics.\n",
    "\n",
    "LocationRaw - The freetext location as provided by the job advertiser.\n",
    "\n",
    "LocationNormalized - Adzuna's normalised location from within our own location tree, interpreted by us based on the raw location.  Our normaliser is not perfect!\n",
    "\n",
    "ContractType - full_time or part_time, interpreted by Adzuna from description or a specific additional field we received from the advertiser.\n",
    "\n",
    "ContractTime - permanent or contract, interpreted by Adzuna from description or a specific additional field we received from the advertiser.\n",
    "\n",
    "Company - the name of the employer as supplied to us by the job advertiser.\n",
    "\n",
    "Category - which of 30 standard job categories this ad fits into, inferred in a very messy way based on the source the ad came from.  We know there is a lot of noise and error in this field.\n",
    "\n",
    "## Desiered field\n",
    "SalaryRaw - the freetext salary field we received in the job advert from the advertiser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame.from_csv('Train_rev1.csv', index_col=None)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.ContractType = X.ContractType.astype(str)\n",
    "X.ContractTime = X.ContractTime.astype(str)\n",
    "X.Company = X.Company.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = X['SalaryNormalized'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del X['Id'], X['Title'],  X['SalaryRaw'], X['SourceName'], X['SalaryNormalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(Y, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "At this very stage we will distill valuable data out of the dataset\n",
    "\n",
    "First of all - let's remove rare tokens and finalaize our dictionary\n",
    "\n",
    "We count all tokens ever occurred in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = list(X.columns)\n",
    "print text_columns\n",
    "categorial_colums = ['ContractType',\n",
    " 'ContractTime',\n",
    " 'Company',\n",
    " 'Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code is below. \n",
    "\n",
    "ember to apply .lower() to all strings before tokenization\n",
    "\n",
    "Consider using tqdm_notebook for not dying  during the iteration process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+|\\d+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenize(value):\n",
    "    return tokenizer.tokenize(value.lower())\n",
    "\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert token_counts.most_common(2) == [('and', 2655140), ('the', 2091168)]\n",
    "assert np.abs(len(token_counts) - 205935) < 20\n",
    "print('Correct!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to actually build token dict. We will use only words that occur more than `min_count` times in dataset\n",
    "\n",
    "Fill two mappings id->token and token->id\n",
    "\n",
    "**Minimum id is 2**, because 0 is reserved for padding and 1 is UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "\n",
    "tokens = <tokens from token_counts keys that had at least min_count occurrences throughout the dataset>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(tokens)==list\n",
    "assert len(tokens)==33497\n",
    "assert 'me' in tokens\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_size = len(tokens)+2\n",
    "id_to_word = dict()\n",
    "word_to_id = dict()\n",
    "\n",
    "# <your code here>\n",
    "token_to_id = {t:i+2 for i,t in enumerate(tokens)}\n",
    "\n",
    "id_to_token = {i+2:t for i,t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert token_to_id['me'] != token_to_id['woods']\n",
    "assert token_to_id[id_to_token[42]]==42\n",
    "assert len(token_to_id)==len(tokens)\n",
    "assert 0 not in id_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, UNK):\n",
    "    '''This function gets a string array and transforms it to padded token matrix\n",
    "    Remembe'''\n",
    "    max_len = 0\n",
    "    token_matrix = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    <YOUR CODE HERE>\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = vectorize([\"Hello, adshkjasdhkas\", \"data\"], token_to_id, 1)\n",
    "assert test.shape==(2,2)\n",
    "assert (test[:,1]==(1,0)).all()\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you successfully completed all tasks by this moment, you ger 3 pts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True deep learning\n",
    "\n",
    "Now we will define our convolutional neural network.\n",
    "\n",
    "We will think about categorical fields as a sequential - but we won't apply CNN to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES = \"\"\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialaize some placeholders for data\n",
    "\n",
    "What size it should  be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "placeholders = dict()\n",
    "for col in text_columns:\n",
    "    placeholders[col] = tf.placeholder(dtype=tf.int32,\n",
    "                                        shape=[None, None],\n",
    "                                        name=\"word_ids_%s\"%col)\n",
    "    \n",
    "true_y = tf.placeholder(dtype=tf.float32, shape = [None], name=\"true_y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are vector represetations for tokens. Basically it is just a table where each token (represented by it's id) has a vector representing its sense.\n",
    "\n",
    "It will be learned simultaneously with other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_size = 50\n",
    "word_embeddings_matrix = tf.get_variable(name=\"word_embeddings_matrix\",\n",
    "                                         dtype=tf.float32,\n",
    "                                         initializer=tf.random_normal(\n",
    "                                             shape=[dict_size, embeddings_size], #<your_code_here>\n",
    "                                             stddev=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep nets\n",
    "Below we are gonna define some network architectures corresponding to each input (a column from a source data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dream_neural_net(word_ids, name):\n",
    "    with tf.variable_scope(name_or_scope=\"conv_\" + name):\n",
    "        output = tf.nn.embedding_lookup(params=word_embeddings_matrix,\n",
    "                                             ids=word_ids)\n",
    "        <YOUR CODE HERE>\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dream_neural_net_categorial(word_ids, name):\n",
    "        word_embeddings = tf.nn.embedding_lookup(params=word_embeddings_matrix,\n",
    "                                             ids=word_ids)\n",
    "         \n",
    "        <YOUR CODE HERE>\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to combine all architectures. In a dict below you can match input type and an architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nets_types = {'FullDescription': dream_neural_net, \n",
    " 'LocationRaw': dream_neural_net, \n",
    " 'LocationNormalized': dream_neural_net, \n",
    " 'ContractType': dream_neural_net_categorial, \n",
    " 'ContractTime': dream_neural_net_categorial, \n",
    " 'Company': dream_neural_net_categorial, \n",
    " 'Category':dream_neural_net_categorial\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_to_concat = [nets_types[name](word_ids, name) for name, word_ids in placeholders.items()]\n",
    "dense_repr = tf.concat(outputs_to_concat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(dense_repr.get_shape().as_list())==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_to_number(dense_inputs):\n",
    "    <YOUR CODE HERE>\n",
    "    output = tf.contrib.layers.fully_connected(inputs=output,\n",
    "                                                   num_outputs=<YOUR CODE HERE>,\n",
    "                                                   activation_fn=None,\n",
    "                                                   weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                   biases_initializer=tf.zeros_initializer(),\n",
    "                                                   scope=\"dense_2\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net_output = reduce_to_number(dense_repr)\n",
    "predicted_y = tf.reshape(net_output, (-1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert net_output.get_shape().as_list()==[None, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Remember - we have a regression task, what would be the loss?\n",
    "\n",
    "Also we will estimate a target metric for a competition - **Mean absolute error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = <YOUR CODE HERE>\n",
    "mean_abs_error = tf.reduce_mean(tf.abs(predicted_y - true_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = <your code here>\n",
    "global_step = tf.Variable(initial_value=0)\n",
    "train_op = optimizer.minimize(\n",
    "  loss=loss,\n",
    "  global_step=global_step, var_list=tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process\n",
    "\n",
    "The last thing before we can run the whole monster - define train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_batches(X, Y=None):\n",
    "    \"\"\"Takes a part of pandas DF\n",
    "    Returns a pair or only X_batch, where X_batch is a dict {key: value} - where a key is name of column, \n",
    "    and a value is a matrix which will be passed to corresponding input\n",
    "    \"\"\"\n",
    "    size = len(X)\n",
    "    i = 0\n",
    "    while i < len(X):\n",
    "        <YOUR CODE HERE>\n",
    "        \n",
    "        \n",
    "        if Y is None:\n",
    "            yield X_batch \n",
    "        else:\n",
    "            yield X_batch, Y_batch\n",
    "        i+=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dict(X_batch,Y_batch=None):\n",
    "    feed_dct = {placeholders[k]: X_batch[k] for k in text_columns} \n",
    "    if Y_batch is not None:\n",
    "        feed_dct[true_y] = Y_batch\n",
    "    return feed_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    MSE, AE = 0, 0\n",
    "    batches = 0\n",
    "    <YOUR CODE HERE>\n",
    "    MSE/=batches\n",
    "    AE/=batches\n",
    "    return (MSE, AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    return sess.run(predicted_y, get_dict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for first_batch in iterate_batches(X_train, Y_train):\n",
    "    break\n",
    "assert len(first_batch) == 2\n",
    "assert type(first_batch[0]) == dict\n",
    "assert first_batch[1].shape[0]==batch_size\n",
    "assert np.unique([inp.shape[0] for inp in first_batch[0].values()])==batch_size\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MSE, AE = validate()\n",
    "assert MSE < 1e10\n",
    "assert AE < 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_batches = len(X_train)/batch_size\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Finally it is time to run training\n",
    "\n",
    "First, shuffle the data\n",
    "\n",
    "\n",
    "By the way, if the trainig processes starts and you achive at leasrt **20k** AE error on validation, you get additional **(3 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE, AE = validate()\n",
    "assert AE < 20000\n",
    "print (\"I earned 3 pts with %s absolute error!\" % AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(X_train))\n",
    "X_train = X_train.iloc[p]\n",
    "Y_train = Y_train.iloc[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(num_epochs):\n",
    "    for i, (X_batch, Y_batch) in  enumerate(iterate_batches(X_train, Y_train)):\n",
    "        _, step, current_loss, abs_error =  sess.run([train_op, global_step, loss, mean_abs_error], get_dict(X_batch,Y_batch))\n",
    "        print \"Current step: %s. Current loss is %s. Absolute error is %s\" % (step, current_loss, abs_error)\n",
    "        if i%30==0:\n",
    "            print(\"Validation. MSE: %s, AE: %s\" %validate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further experiments and home assignment\n",
    " - **(1 pt)** Add visualisation of train and val loss. Try to use some smoothing to make plots more readable \n",
    " - **(1 pt)** Try different CNN architectures. Vary kernel size, number of filters. Find out if there is some change to a training process.\n",
    " - Try to use different architectures for different inputs. Maybe a smaller architecture would be fine for description field and more complex for a title.\n",
    " - Find out the best **embedding size** value \n",
    " - Add dropout, and some regularisation \n",
    " \n",
    " **(2++ pts) for all experimenting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See in the next series\n",
    " - Recurrent neural networks\n",
    "  - Why everybody like them\n",
    "  - Why everybody hate them\n",
    " - Attention for text processing\n",
    "  - How to boost your NLP model performance by implementing recent DL paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
