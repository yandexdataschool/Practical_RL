{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seminar 9: exploration vs exploitation\n",
    "\n",
    "In this seminar, we'll employ bayesian neural networks to facilitate exploration in contextual bandits.\n",
    "\n",
    "__About bayesian neural networks:__\n",
    "  * A post on the matter - [url](http://twiecki.github.io/blog/2016/07/05/bayesian-deep-learning/)\n",
    "  * Theano+PyMC3 for more serious stuff - [url](http://pymc-devs.github.io/pymc3/notebooks/bayesian_neural_network_advi.html)\n",
    "  * Same stuff in tensorflow - [url](http://edwardlib.org/tutorials/bayesian-neural-network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "In this seminar, we're going to solve a toy contextual bandit problem\n",
    "* 60-dimensional states\n",
    "* 10 actions\n",
    "* rewards between 0 and 1\n",
    "\n",
    "Instead of actually running on a stream of data, we're gonna emulate it with samples from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60d states, 10 actions\n"
     ]
    }
   ],
   "source": [
    "all_states = np.load(\"all_states.npy\")\n",
    "action_rewards = np.load(\"action_rewards.npy\")\n",
    "\n",
    "state_size = all_states.shape[1]\n",
    "n_actions = action_rewards.shape[1]\n",
    "print (\"%id states, %i actions\"%(state_size,n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano, theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne import init\n",
    "from lasagne.layers import *\n",
    "import bayes\n",
    "as_bayesian = bayes.bbpwrap(bayes.NormalApproximation(std=0.1))\n",
    "BayesDenseLayer = as_bayesian(DenseLayer)\n",
    "#similar: BayesConv2DLayer = as_bayesian(Conv2DLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    \"\"\"a bandit with bayesian neural net\"\"\"\n",
    "    def __init__(self,state_size=state_size,n_actions=n_actions):\n",
    "        \n",
    "        #input variables\n",
    "        input_states = T.matrix(\"states\")\n",
    "        target_actions = T.ivector(\"actions taken\")\n",
    "        target_rewards = T.vector(\"rewards\")\n",
    "        \n",
    "        self.total_samples_seen = theano.shared(np.int32(0),\"number of training samples seen so far\")\n",
    "        \n",
    "        batch_size = target_actions.shape[0]\n",
    "\n",
    "        \n",
    "        ###\n",
    "        #network body\n",
    "        inp = InputLayer((None,state_size),name='input')\n",
    "\n",
    "        hid = <create bayesian dense layer for hidden states>\n",
    "        \n",
    "        out = <create bayesian dense layer that predicts Q's aka actions>\n",
    "\n",
    "        ###\n",
    "        #prediction\n",
    "        \n",
    "        prediction_all_actions = get_output(out,inputs=input_states)\n",
    "        \n",
    "        self.predict_sample_rewards = theano.function([input_states],prediction_all_actions)\n",
    "\n",
    "        ###\n",
    "        #Training\n",
    "\n",
    "        #select prediction for target action\n",
    "        prediction_target_actions = prediction_all_actions[T.arange(batch_size),target_actions]\n",
    "\n",
    "        #loss = negative log-likelihood (mse) + KL\n",
    "        negative_llh = T.sum((prediction_target_actions - target_rewards)**2) \n",
    "        \n",
    "        kl = bayes.get_var_cost(out) / (self.total_samples_seen+batch_size)\n",
    "        \n",
    "        loss = (negative_llh + kl)/batch_size\n",
    "\n",
    "        self.weights = get_all_params(out,trainable=True)\n",
    "        self.out=out\n",
    "\n",
    "        #gradient descent\n",
    "        updates = lasagne.updates.adam(loss,self.weights)\n",
    "        #update counts\n",
    "        updates[self.total_samples_seen]=self.total_samples_seen+batch_size.astype('int32')\n",
    "        \n",
    "        self.train_step = theano.function([input_states,target_actions,target_rewards],\n",
    "                                          [negative_llh,kl],updates = updates,\n",
    "                                          allow_input_downcast=True)\n",
    "    \n",
    "    \n",
    "    def sample_prediction(self,states,n_samples=1):\n",
    "        \"\"\"Samples n_samples predictions for rewards,\n",
    "        \n",
    "        :returns: tensor [n_samples,state_i,action_i]\n",
    "        \"\"\"\n",
    "        assert states.ndim==2,\"states must be 2-dimensional\"\n",
    "        \n",
    "        return np.stack([self.predict_sample_rewards(states) for _ in range(n_samples)])\n",
    "    \n",
    "    \n",
    "    epsilon=0.25\n",
    "    def get_action(self,states):\n",
    "        \"\"\"\n",
    "        Picks action by \n",
    "        - with p=1-epsilon, taking argmax of average rewards\n",
    "        - with p=epsilon, taking random action\n",
    "        This is exactly e-greedy policy.\n",
    "        \"\"\"\n",
    "        \n",
    "        reward_samples = self.sample_prediction(states,n_samples=100)\n",
    "        #^-- samples for rewards, shape = [n_samples,n_states,n_actions]\n",
    "        \n",
    "        best_actions = reward_samples.mean(axis=0).argmax(axis=-1)\n",
    "        #^-- we take mean over samples to compute expectation, then pick best action with argmax \n",
    "        \n",
    "        random_actions = <generate random actions>\n",
    "        \n",
    "        chosen_actions = <pick actions with e-greedy policy>\n",
    "        \n",
    "        return chosen_actions\n",
    "    \n",
    "    def train(self,states,actions,rewards,n_iters=10):\n",
    "        \"\"\"\n",
    "        trains to predict rewards for chosen actions in given states\n",
    "        \"\"\"\n",
    "        \n",
    "        loss_sum = kl_sum = 0\n",
    "        for _ in range(n_iters):\n",
    "            loss,kl = self.train_step(states,actions,rewards)\n",
    "            loss_sum += loss\n",
    "            kl_sum += kl\n",
    "\n",
    "            \n",
    "        return loss_sum/n_iters,kl_sum/n_iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the bandit\n",
    "\n",
    "We emulate infinite stream of data and pick actions using agent's get_action function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bandit = Bandit() #create your bandit\n",
    "\n",
    "rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_samples(states,action_rewards,batch_size=10):\n",
    "    \"\"\"samples random minibatch, emulating new users\"\"\"\n",
    "    batch_ix = np.random.randint(0,len(states),batch_size)\n",
    "    return states[batch_ix],action_rewards[batch_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from pandas import ewma\n",
    "\n",
    "batch_size=10 #10 new users\n",
    "\n",
    "for i in range(1000):\n",
    "    ###\n",
    "    #new data\n",
    "    b_states,b_action_rewards = get_new_samples(all_states,action_rewards,batch_size)\n",
    "    \n",
    "    ###\n",
    "    #pick actions\n",
    "    b_actions = bandit.get_action(b_states)\n",
    "    \n",
    "    ###\n",
    "    #rewards for actions agent just took\n",
    "    b_rewards = b_action_rewards[np.arange(batch_size),b_actions]\n",
    "    \n",
    "    ###\n",
    "    #train bandit\n",
    "    mse,kl = bandit.train(b_states,b_actions,b_rewards,n_iters=100)\n",
    "    \n",
    "    rewards_history.append(b_rewards.mean())\n",
    "    \n",
    "    if i%10 ==0:\n",
    "        clear_output(True)\n",
    "        print(\"iteration #%i\\tmean reward=%.3f\\tmse=%.3f\\tkl=%.3f\"%(i,np.mean(rewards_history[-10:]),mse,kl))\n",
    "        plt.plot(rewards_history)\n",
    "        plt.plot(ewma(np.array(rewards_history),alpha=0.1))\n",
    "        plt.show()\n",
    "        samples = bandit.sample_prediction(b_states[:1],n_samples=100).T[:,0,:]\n",
    "        for i in range(len(samples)):\n",
    "            plt.hist(samples[i],alpha=0.25,label=str(i))\n",
    "            plt.legend(loc='best')\n",
    "        print('Q(s,a) std:', ';'.join(list(map('{:.3f}'.format,np.std(samples,axis=1)))))\n",
    "        print('correct',b_action_rewards[0].argmax())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better exploration\n",
    "\n",
    "You will now implement the two exploration strategies from the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#then implement it and replace bandit = Bandit() above with ThompsonBandit()\n",
    "class ThompsonBandit(Bandit):\n",
    "    def get_action(self,states):\n",
    "        \"\"\"\n",
    "        picks action based by taking _one_ sample from BNN and taking action with highest sampled reward (yes, that simple)\n",
    "        This is exactly thompson sampling.\n",
    "        \"\"\"\n",
    "        <your code>\n",
    "        \n",
    "        return <your code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#then implement it and replace bandit = Bandit() above with UCBBandit()\n",
    "\n",
    "class UCBBandit(Bandit):\n",
    "    q = 90\n",
    "    def get_action(self,states):\n",
    "        \"\"\"\n",
    "        Compute q-th percentile of rewards P(r|s,a) for all actions\n",
    "        Take actions that have highest percentiles.\n",
    "        \n",
    "        This implements bayesian UCB strategy\n",
    "        \"\"\"\n",
    "        \n",
    "        <Your code here>\n",
    "        \n",
    "        return <actions with bayesian ucb>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bandit = <UCBBandit or ThompsonBandit>\n",
    "#<maybe change parameters>\n",
    "\n",
    "rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from pandas import ewma\n",
    "\n",
    "batch_size=10 #10 new users\n",
    "\n",
    "for i in range(1000):\n",
    "    ###\n",
    "    #new data\n",
    "    b_states,b_action_rewards = get_new_samples(all_states,action_rewards,batch_size)\n",
    "    \n",
    "    ###\n",
    "    #pick actions\n",
    "    b_actions = bandit.get_action(b_states)\n",
    "    \n",
    "    ###\n",
    "    #rewards for actions agent just took\n",
    "    b_rewards = b_action_rewards[np.arange(batch_size),b_actions]\n",
    "    \n",
    "    ###\n",
    "    #train bandit\n",
    "    mse,kl = bandit.train(b_states,b_actions,b_rewards,n_iters=100)\n",
    "    \n",
    "    rewards_history.append(b_rewards.mean())\n",
    "    \n",
    "    if i%10 ==0:\n",
    "        clear_output(True)\n",
    "        print(\"iteration #%i\\tmean reward=%.3f\\tmse=%.3f\\tkl=%.3f\"%(i,np.mean(rewards_history[-10:]),mse,kl))\n",
    "        plt.plot(rewards_history)\n",
    "        plt.plot(ewma(np.array(rewards_history),alpha=0.1))\n",
    "        plt.show()\n",
    "        samples = bandit.sample_prediction(b_states[:1],n_samples=100).T[:,0,:]\n",
    "        for i in range(len(samples)):\n",
    "            plt.hist(samples[i],alpha=0.25,label=str(i))\n",
    "            plt.legend(loc='best')\n",
    "        print('Q(s,a) std:', ';'.join(list(map('{:.3f}'.format,np.std(samples,axis=1)))))\n",
    "        print('correct',b_action_rewards[0].argmax())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Experience replay\n",
    "\n",
    "Our value-based bandit algorithm is off-policy, so we can train it on actions from a different policy.\n",
    "\n",
    "For example, the bandit will need much less interactions to converge if you train it on past experiences. You can also pre-train it on any data you already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayBandit(Bandit): #or your preferred exploration type\n",
    "    \"\"\"A bandit that trains not on last user interactions but on random samples from everything it saw\"\"\"\n",
    "    experience_buffer=[]\n",
    "    \n",
    "    <Your code here. You will at least need to modify train function>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
