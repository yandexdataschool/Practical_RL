{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["### Seminar 9: exploration vs exploitation\n", "\n", "In this seminar, we'll employ bayesian neural networks to facilitate exploration in contextual bandits.\n", "\n", "__About bayesian neural networks:__\n", "  * A post on the matter - [url](http://twiecki.github.io/blog/2016/07/05/bayesian-deep-learning/)\n", "  * Theano+PyMC3 for more serious stuff - [url](http://pymc-devs.github.io/pymc3/notebooks/bayesian_neural_network_advi.html)\n", "  * Same stuff in tensorflow - [url](http://edwardlib.org/tutorials/bayesian-neural-network)\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"collapsed": true}, "outputs": [], "source": ["import numpy as np", "\n", "import matplotlib.pyplot as plt", "\n", "%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Load data\n", "\n", "In this seminar, we're going to solve a toy contextual bandit problem\n", "* 60-dimensional states\n", "* 10 actions\n", "* rewards between 0 and 1\n", "\n", "Instead of actually running on a stream of data, we're gonna emulate it with samples from dataset."]}, {"cell_type": "code", "execution_count": 5, "metadata": {"collapsed": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["60d states, 10 actions\n"]}], "source": ["all_states = np.load(\"all_states.npy\")", "\n", "action_rewards = np.load(\"action_rewards.npy\")", "\n", "\n", "state_size = all_states.shape[1]", "\n", "n_actions = action_rewards.shape[1]", "\n", "print(\"%id states, %i actions\" % (state_size, n_actions))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["import theano", "\n", "import theano.tensor as T", "\n", "import lasagne", "\n", "from lasagne import init", "\n", "from lasagne.layers import *", "\n", "import bayes", "\n", "as_bayesian = bayes.bbpwrap(bayes.NormalApproximation(std=0.1))", "\n", "BayesDenseLayer = as_bayesian(DenseLayer)", "\n", "#similar: BayesConv2DLayer = as_bayesian(Conv2DLayer)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["class Bandit:", "\n", "    \"\"\"a bandit with bayesian neural net\"\"\"", "\n", "\n", "    def __init__(self, state_size=state_size, n_actions=n_actions):", "\n", "\n", "        # input variables", "\n", "        input_states = T.matrix(\"states\")", "\n", "        target_actions = T.ivector(\"actions taken\")", "\n", "        target_rewards = T.vector(\"rewards\")", "\n", "\n", "        self.total_samples_seen = theano.shared(", "\n", "            np.int32(0), \"number of training samples seen so far\")", "\n", "\n", "        batch_size = target_actions.shape[0]", "\n", "\n", "        ###", "\n", "        # network body", "\n", "        inp = InputLayer((None, state_size), name='input')", "\n", "\n", "        hid = <create bayesian dense layer for hidden states >", "\n", "\n", "        out = <create bayesian dense layer that predicts Q's aka actions >", "\n", "\n", "        ###", "\n", "        # prediction", "\n", "\n", "        prediction_all_actions = get_output(out, inputs=input_states)", "\n", "\n", "        self.predict_sample_rewards = theano.function(", "\n", "            [input_states], prediction_all_actions)", "\n", "\n", "        ###", "\n", "        # Training", "\n", "\n", "        # select prediction for target action", "\n", "        prediction_target_actions = prediction_all_actions[T.arange(", "\n", "            batch_size), target_actions]", "\n", "\n", "        # loss = negative log-likelihood (mse) + KL", "\n", "        negative_llh = T.sum((prediction_target_actions - target_rewards)**2)", "\n", "\n", "        kl = bayes.get_var_cost(out) / (self.total_samples_seen+batch_size)", "\n", "\n", "        loss = (negative_llh + kl)/batch_size", "\n", "\n", "        self.weights = get_all_params(out, trainable=True)", "\n", "        self.out = out", "\n", "\n", "        # gradient descent", "\n", "        updates = lasagne.updates.adam(loss, self.weights)", "\n", "        # update counts", "\n", "        updates[self.total_samples_seen] = self.total_samples_seen + \\", "\n", "            batch_size.astype('int32')", "\n", "\n", "        self.train_step = theano.function([input_states, target_actions, target_rewards],", "\n", "                                          [negative_llh, kl], updates=updates,", "\n", "                                          allow_input_downcast=True)", "\n", "\n", "    def sample_prediction(self, states, n_samples=1):", "\n", "        \"\"\"Samples n_samples predictions for rewards,", "\n", "\n", "        :returns: tensor [n_samples,state_i,action_i]", "\n", "        \"\"\"", "\n", "        assert states.ndim == 2, \"states must be 2-dimensional\"", "\n", "\n", "        return np.stack([self.predict_sample_rewards(states) for _ in range(n_samples)])", "\n", "\n", "    epsilon = 0.25", "\n", "\n", "    def get_action(self, states):", "\n", "        \"\"\"", "\n", "        Picks action by ", "\n", "        - with p=1-epsilon, taking argmax of average rewards", "\n", "        - with p=epsilon, taking random action", "\n", "        This is exactly e-greedy policy.", "\n", "        \"\"\"", "\n", "\n", "        reward_samples = self.sample_prediction(states, n_samples=100)", "\n", "        #^-- samples for rewards, shape = [n_samples,n_states,n_actions]", "\n", "\n", "        best_actions = reward_samples.mean(axis=0).argmax(axis=-1)", "\n", "        #^-- we take mean over samples to compute expectation, then pick best action with argmax", "\n", "\n", "        random_actions = <generate random actions >", "\n", "\n", "        chosen_actions = <pick actions with e-greedy policy >", "\n", "\n", "        return chosen_actions", "\n", "\n", "    def train(self, states, actions, rewards, n_iters=10):", "\n", "        \"\"\"", "\n", "        trains to predict rewards for chosen actions in given states", "\n", "        \"\"\"", "\n", "\n", "        loss_sum = kl_sum = 0", "\n", "        for _ in range(n_iters):", "\n", "            loss, kl = self.train_step(states, actions, rewards)", "\n", "            loss_sum += loss", "\n", "            kl_sum += kl", "\n", "\n", "        return loss_sum/n_iters, kl_sum/n_iters"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Train the bandit\n", "\n", "We emulate infinite stream of data and pick actions using agent's get_action function."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["bandit = Bandit()  # create your bandit", "\n", "\n", "rewards_history = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["def get_new_samples(states, action_rewards, batch_size=10):", "\n", "    \"\"\"samples random minibatch, emulating new users\"\"\"", "\n", "    batch_ix = np.random.randint(0, len(states), batch_size)", "\n", "    return states[batch_ix], action_rewards[batch_ix]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["from IPython.display import clear_output", "\n", "from pandas import ewma", "\n", "\n", "batch_size = 10  # 10 new users", "\n", "\n", "for i in range(1000):", "\n", "    ###", "\n", "    # new data", "\n", "    b_states, b_action_rewards = get_new_samples(", "\n", "        all_states, action_rewards, batch_size)", "\n", "\n", "    ###", "\n", "    # pick actions", "\n", "    b_actions = bandit.get_action(b_states)", "\n", "\n", "    ###", "\n", "    # rewards for actions agent just took", "\n", "    b_rewards = b_action_rewards[np.arange(batch_size), b_actions]", "\n", "\n", "    ###", "\n", "    # train bandit", "\n", "    mse, kl = bandit.train(b_states, b_actions, b_rewards, n_iters=100)", "\n", "\n", "    rewards_history.append(b_rewards.mean())", "\n", "\n", "    if i % 10 == 0:", "\n", "        clear_output(True)", "\n", "        print(\"iteration #%i\\tmean reward=%.3f\\tmse=%.3f\\tkl=%.3f\" %", "\n", "              (i, np.mean(rewards_history[-10:]), mse, kl))", "\n", "        plt.plot(rewards_history)", "\n", "        plt.plot(ewma(np.array(rewards_history), alpha=0.1))", "\n", "        plt.show()", "\n", "        samples = bandit.sample_prediction(", "\n", "            b_states[:1], n_samples=100).T[:, 0, :]", "\n", "        for i in range(len(samples)):", "\n", "            plt.hist(samples[i], alpha=0.25, label=str(i))", "\n", "            plt.legend(loc='best')", "\n", "        print('Q(s,a) std:', ';'.join(", "\n", "            list(map('{:.3f}'.format, np.std(samples, axis=1)))))", "\n", "        print('correct', b_action_rewards[0].argmax())", "\n", "        plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Better exploration\n", "\n", "You will now implement the two exploration strategies from the lecture."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# then implement it and replace bandit = Bandit() above with ThompsonBandit()", "\n", "class ThompsonBandit(Bandit):", "\n", "    def get_action(self, states):", "\n", "        \"\"\"", "\n", "        picks action based by taking _one_ sample from BNN and taking action with highest sampled reward (yes, that simple)", "\n", "        This is exactly thompson sampling.", "\n", "        \"\"\"", "\n", "        <your code >", "\n", "\n", "        return < your code >"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# then implement it and replace bandit = Bandit() above with UCBBandit()", "\n", "\n", "\n", "class UCBBandit(Bandit):", "\n", "    q = 90", "\n", "\n", "    def get_action(self, states):", "\n", "        \"\"\"", "\n", "        Compute q-th percentile of rewards P(r|s,a) for all actions", "\n", "        Take actions that have highest percentiles.", "\n", "\n", "        This implements bayesian UCB strategy", "\n", "        \"\"\"", "\n", "\n", "        <Your code here >", "\n", "\n", "        return < actions with bayesian ucb >"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["bandit = <UCBBandit or ThompsonBandit >", "\n", "#<maybe change parameters>", "\n", "\n", "rewards_history = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["from IPython.display import clear_output", "\n", "from pandas import ewma", "\n", "\n", "batch_size = 10  # 10 new users", "\n", "\n", "for i in range(1000):", "\n", "    ###", "\n", "    # new data", "\n", "    b_states, b_action_rewards = get_new_samples(", "\n", "        all_states, action_rewards, batch_size)", "\n", "\n", "    ###", "\n", "    # pick actions", "\n", "    b_actions = bandit.get_action(b_states)", "\n", "\n", "    ###", "\n", "    # rewards for actions agent just took", "\n", "    b_rewards = b_action_rewards[np.arange(batch_size), b_actions]", "\n", "\n", "    ###", "\n", "    # train bandit", "\n", "    mse, kl = bandit.train(b_states, b_actions, b_rewards, n_iters=100)", "\n", "\n", "    rewards_history.append(b_rewards.mean())", "\n", "\n", "    if i % 10 == 0:", "\n", "        clear_output(True)", "\n", "        print(\"iteration #%i\\tmean reward=%.3f\\tmse=%.3f\\tkl=%.3f\" %", "\n", "              (i, np.mean(rewards_history[-10:]), mse, kl))", "\n", "        plt.plot(rewards_history)", "\n", "        plt.plot(ewma(np.array(rewards_history), alpha=0.1))", "\n", "        plt.show()", "\n", "        samples = bandit.sample_prediction(", "\n", "            b_states[:1], n_samples=100).T[:, 0, :]", "\n", "        for i in range(len(samples)):", "\n", "            plt.hist(samples[i], alpha=0.25, label=str(i))", "\n", "            plt.legend(loc='best')", "\n", "        print('Q(s,a) std:', ';'.join(", "\n", "            list(map('{:.3f}'.format, np.std(samples, axis=1)))))", "\n", "        print('correct', b_action_rewards[0].argmax())", "\n", "        plt.show()"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["### Experience replay\n", "\n", "Our value-based bandit algorithm is off-policy, so we can train it on actions from a different policy.\n", "\n", "For example, the bandit will need much less interactions to converge if you train it on past experiences. You can also pre-train it on any data you already have."]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["class ReplayBandit(Bandit):  # or your preferred exploration type", "\n", "    \"\"\"A bandit that trains not on last user interactions but on random samples from everything it saw\"\"\"", "\n", "    experience_buffer = []", "\n", "\n", "    <Your code here. You will at least need to modify train function >"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```\n", "\n", "```"]}], "metadata": {"kernelspec": {"display_name": "Python 2", "language": "python", "name": "python2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 2}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython2", "version": "2.7.13"}}, "nbformat": 4, "nbformat_minor": 1}